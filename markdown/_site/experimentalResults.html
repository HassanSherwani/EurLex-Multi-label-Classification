<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/united.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #0000ff; } /* Keyword */
code > span.ch { color: #008080; } /* Char */
code > span.st { color: #008080; } /* String */
code > span.co { color: #008000; } /* Comment */
code > span.ot { color: #ff4000; } /* Other */
code > span.al { color: #ff0000; } /* Alert */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #008000; font-weight: bold; } /* Warning */
code > span.cn { } /* Constant */
code > span.sc { color: #008080; } /* SpecialChar */
code > span.vs { color: #008080; } /* VerbatimString */
code > span.ss { color: #008080; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { } /* Variable */
code > span.cf { color: #0000ff; } /* ControlFlow */
code > span.op { } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #ff4000; } /* Preprocessor */
code > span.do { color: #008000; } /* Documentation */
code > span.an { color: #008000; } /* Annotation */
code > span.cv { color: #008000; } /* CommentVar */
code > span.at { } /* Attribute */
code > span.in { color: #008000; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4,h5,h6",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = false;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">EurLex classification</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="multilabel_classification_intro.html">Multi-label classification</a>
</li>
<li>
  <a href="exploratory_analysis.html">Exploratory Analysis</a>
</li>
<li>
  <a href="experimentalResults.html">Classification</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<p>     </p>
<div id="classification-task" class="section level2">
<h2>Classification task</h2>
<p>The EUR-Lex dataset contains 25K documents, which makes it impossible to train a classifier over the whole dataset. We divided the dataset into 24 subsets, each subset contains the same number of documents, we trained the classifiers over the subsets. The final evaluations results will be the average seperatly reported results for each subset. The dataset has around 7000 labels, an older version had only 4000 labels. To observe the effect of including a large number of labels on the classification task in general, we compared the predective performance of the models when trained on a dataset with 7000 labels and when trained on a dataset with a reduced number of labels.</p>
<div id="classification-models" class="section level4">
<h4><span class="sub-sub-header">Classification Models</span></h4>
<p>We use three methods to transform the problem of multi-label classification task into a conventional multi-class classification task: binary relevance, label powerset, classifier chain. After transforming the classification problem, We trained three different classification models: Random Forest, k nearest neighbors, XGboosted trees.</p>
<table class="kable_wrapper table table-striped table-hover table-condensed" style="width: auto !important; margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td>
<table>
<thead>
<tr>
<th style="text-align:left;">
classifier Models
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
———————-
</td>
</tr>
<tr>
<td style="text-align:left;">
K nearest neighbours
</td>
</tr>
<tr>
<td style="text-align:left;">
XGboost
</td>
</tr>
<tr>
<td style="text-align:left;">
Random Forest
</td>
</tr>
</tbody>
</table>
</td>
<td>
<table>
<thead>
<tr>
<th style="text-align:left;">
Transformation methods
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
———————-
</td>
</tr>
<tr>
<td style="text-align:left;">
Binary relevance
</td>
</tr>
<tr>
<td style="text-align:left;">
Classifier chain
</td>
</tr>
<tr>
<td style="text-align:left;">
Label powerset
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>We trained nine classifaction models and compared the performance of the classifiers to produce the model that best fits the dataset and to choose the type of features to yield the best predictive performance. The following table shows the nine classifier models :</p>
<table class="table table-striped table-hover table-condensed table-responsive" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td style="text-align:left;">
KNN-Label Powerset
</td>
<td style="text-align:left;">
RF-Label Powerset
</td>
<td style="text-align:left;">
XGboost-Label Powerset
</td>
</tr>
</tbody>
</table>
<table class="table table-striped table-hover table-condensed table-responsive" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td style="text-align:left;">
KNN-Binary Relevance
</td>
<td style="text-align:left;">
RF-Binary Relevance
</td>
<td style="text-align:left;">
XGboost-Binary Relevance
</td>
</tr>
</tbody>
</table>
<table class="table table-striped table-hover table-condensed table-responsive" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td style="text-align:left;">
KNN-Classifier Chain
</td>
<td style="text-align:left;">
RF-Classifier Chain
</td>
<td style="text-align:left;">
XGboost-Classifier Chain
</td>
</tr>
</tbody>
</table>
</div>
<div id="experimental-settings" class="section level3">
<h3><span class="sub-sub-header">Experimental settings</span></h3>
<p>As mentioned earlier, the 25K documents dataset was split into 24 subsets.Each subset was split randomly into two disjointly subsets one for training and the other for testing, with the following proportions ( 65% used for training and 35% used for testing). We trained the previously mentioned classification models separately in order to compare their performance. We reported the results of the different models under different settings, we wanted to explore the performance of the classifiers with two types of features, with two languages and with different number of labelsets. The following table demonstrates the experimental settings:</p>
<table class="kable_wrapper table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td>
<table>
<thead>
<tr>
<th style="text-align:left;">
Language
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
——–
</td>
</tr>
<tr>
<td style="text-align:left;">
English dataset
</td>
</tr>
<tr>
<td style="text-align:left;">
German dataset
</td>
</tr>
</tbody>
</table>
</td>
<td>
<table>
<thead>
<tr>
<th style="text-align:left;">
Features
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
——–
</td>
</tr>
<tr>
<td style="text-align:left;">
TFIDF
</td>
</tr>
<tr>
<td style="text-align:left;">
incidence of terms
</td>
</tr>
</tbody>
</table>
</td>
<td>
<table>
<thead>
<tr>
<th style="text-align:left;">
Number of Labelsets
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
——–
</td>
</tr>
<tr>
<td style="text-align:left;">
14517
</td>
</tr>
<tr>
<td style="text-align:left;">
only balanced labelsets
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>The following code shows the functions used for classification, we use <a href="https://cran.r-project.org/web/packages/utiml/index.html"><em>utiml</em></a> package. To split the dataset into training and testing set we use the function <a href="https://www.rdocumentation.org/packages/utiml/versions/0.1.4/topics/create_holdout_partition">create_holdout_partition()</a>.The example code is showen for <strong>binary relevance</strong> method. We simply change the method used into one of the following methods: <a href="https://rdrr.io/cran/utiml/man/lp.html">lp()</a> Label Powerset, <a href="https://rdrr.io/cran/utiml/man/br.html">br()</a> Binary Relevance, <a href="https://rdrr.io/cran/utiml/man/cc.html">cc()</a> Classifier Chain we had to destroy the model after exporting the performance results to avoid any problematics with memory capacity using the function <em>rm()</em></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(mldr)
<span class="kw">library</span>(utiml)

train_ratio &lt;-<span class="st"> </span><span class="fl">0.65</span>
test_ratio &lt;-<span class="st"> </span><span class="fl">0.35</span>
iteration &lt;-<span class="st"> </span><span class="dv">24</span>

<span class="cf">for</span>(index <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>iteration){

  ds &lt;-<span class="st"> </span><span class="kw">mldr</span>(<span class="kw">paste</span>(generic_name,index,<span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">remove_attributes</span>(<span class="st">&quot;...&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">remove_unique_attributes</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">remove_unlabeled_instances</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">create_holdout_partition</span>(<span class="kw">c</span>(<span class="dt">train=</span>train_ratio, <span class="dt">test=</span>test_ratio))
  
  ## KNN - K nearest neighbour
  brmodel1 &lt;-<span class="st"> </span><span class="kw">br</span>(ds<span class="op">$</span>train, <span class="st">&quot;KNN&quot;</span>)
  prediction1 &lt;-<span class="st"> </span><span class="kw">predict</span>(brmodel1, ds<span class="op">$</span>test)
  temp_knn &lt;-<span class="st"> </span><span class="kw">multilabel_evaluate</span>(ds<span class="op">$</span>test, prediction1, <span class="st">&quot;bipartition&quot;</span>)

  ##remove model of memory
  <span class="kw">rm</span>(brmodel1)
  <span class="kw">rm</span>(prediction1)
  
  ## RF - Random Forest
  brmodel2 &lt;-<span class="st"> </span><span class="kw">br</span>(ds<span class="op">$</span>train, <span class="st">&quot;RF&quot;</span>)
  prediction2 &lt;-<span class="st"> </span><span class="kw">predict</span>(brmodel2, ds<span class="op">$</span>test)
  temp_rf &lt;-<span class="st"> </span><span class="kw">multilabel_evaluate</span>(ds<span class="op">$</span>test, prediction2, <span class="st">&quot;bipartition&quot;</span>)

  <span class="kw">rm</span>(brmodel2)
  <span class="kw">rm</span>(prediction2)

  ## XGB - eXtreme Gradient Boosting
  brmodel3 &lt;-<span class="st"> </span><span class="kw">br</span>(ds<span class="op">$</span>train, <span class="st">&quot;XGB&quot;</span>)
  prediction3 &lt;-<span class="st"> </span><span class="kw">predict</span>(brmodel3, ds<span class="op">$</span>test)
  temp_xgb &lt;-<span class="st"> </span><span class="kw">multilabel_evaluate</span>(ds<span class="op">$</span>test, prediction3, <span class="st">&quot;bipartition&quot;</span>)

  <span class="kw">rm</span>(brmodel3)
  <span class="kw">rm</span>(prediction3)
  <span class="cf">if</span>(index<span class="op">==</span><span class="dv">1</span>){
  knn &lt;-<span class="st"> </span>temp_knn
  rf &lt;-<span class="st"> </span>temp_rf
  xgb &lt;-<span class="st"> </span>temp_xgb
  }<span class="cf">else</span>{
  knn &lt;-<span class="st"> </span><span class="kw">cbind</span>(knn,temp_knn)
  rf &lt;-<span class="st"> </span><span class="kw">cbind</span>(rf,temp_rf)
  xgb &lt;-<span class="st"> </span><span class="kw">cbind</span>(xgb, temp_xgb)
  }</code></pre></div>
</div>
<div id="experimental-results" class="section level3">
<h3><span class="sub-header">Experimental Results</span></h3>
<p>We tested the nine models over two languages (English and German) and with two types of features (TF-IDF and the terms incidence).<br />
For the purpose of the evaluation task, The <strong>mldr</strong> package equiped us with various measurment tools, we will present all of them for each experiment, however, we chose marco F1 measurement(a measure combines between the precision and the recall) to compare the predective performance of all classifiers. Through the exploration process of the Eur-Lex dataset, the study revealed the class labels to be imbalanced (i.e some labels are frequent and some are infrequent). In that case considering accuracy is a misleading measure of the performance, instead we consider <strong>macro F1</strong> as a comparision factor among the classifiers. In General, across all the experiments, we inferred that combining the label powerset transformation method with the random forest classifier produced the best performance, however, combining the label powerset with the XGBoost classifier resulted in the worst performance.</p>
<div id="dataset-english-dataset" class="section level4">
<h4><span class="sub-header">Dataset: English dataset</span></h4>
<p>For the English dataset, we observed higher macro F1 rates over all the nine trained classifiers when we used TF-IDF as the instances features. TF-IDFs are more powerful representative features than simply using the incidence of terms as features. The expreiments show that label powerset combined with the random forest recorded the best result for the two type of features(TF-IDF and Incidence), whereas the label powerset method combined with XGB performed the worst as shown in the following figures:</p>
<div class="figure">
<img src="Figs/English_7000.png" />

</div>
<div class="figure">
<img src="Figs/English_7000_table.png" />

</div>
<p>In the following sections we will display the results in detail for each feature separately.</p>
</div>
<div id="dataset-english-dataset-feature-tf-idf" class="section level4 tabset tabset-fade">
<h4><span class="sub-sub-header">Dataset: English dataset, Feature: TF-IDF</span></h4>
<div id="label-powerset" class="section level5">
<h5>Label Powerset</h5>
<p>According to the following figures, the label powerset with the k-nearest neighbour and the random forest performed the best, however XGBoost performed the worst.</p>
<div class="figure">
<img src="Figs/lp_EN_tfidf1.png" />

</div>
<div class="figure">
<img src="Figs/lp_EN_tfidf2.png" />

</div>
</div>
<div id="binary-relevance" class="section level5">
<h5>Binary Relevance</h5>
<p>We compared the three models where the labels are assumed to be independent. The binary relevance method produced the best results when combined with XGBoost.</p>
<div class="figure">
<img src="Figs/br_EN_tfidf1.png" />

</div>
<div class="figure">
<img src="Figs/br_EN_tfidf2.png" />

</div>
</div>
<div id="classifier-chain" class="section level5">
<h5>Classifier Chain</h5>
<p>Similar to the binary relevance method, classifier chain method performed the best with the XGBoost classifier.</p>
<div class="figure">
<img src="Figs/cc_EN_tfidf1.png" />

</div>
<div class="figure">
<img src="Figs/cc_EN_tfidf2.png" />

</div>
</div>
</div>
<div id="dataset-english-dataset-feature-incidence-of-terms" class="section level4 tabset tabset-fade">
<h4><span class="sub-sub-header">Dataset: English dataset, Feature: Incidence of terms</span></h4>
<p>We wanted to test the performance of the models in case of employing more naive features as the incidence of terms. For English language, the experiments showed similar pattern as the case in experiments conducted for TF-IDF features, where the label powerset with the random forest classifier scored the highest macro F1 value, with the XGBoost performing the worst. Nevertheless, classifiers trained on the incidence of terms as features did not score better than the ones trained on the TF-IDF features.</p>
<div id="label-powerset-1" class="section level5">
<h5>Label Powerset</h5>
<div class="figure">
<img src="Figs/lp_EN_inc1.png" />

</div>
<div class="figure">
<img src="Figs/lp_EN_inc2.png" />

</div>
</div>
<div id="binary-relevance-1" class="section level5">
<h5>Binary Relevance</h5>
<div class="figure">
<img src="Figs/br_EN_inc1.png" />

</div>
<div class="figure">
<img src="Figs/br_EN_inc2.png" />

</div>
</div>
<div id="classifier-chain-1" class="section level5">
<h5>Classifier Chain</h5>
<div class="figure">
<img src="Figs/cc_EN_inc1.png" />

</div>
<div class="figure">
<img src="Figs/cc_EN_inc2.png" />

</div>
</div>
</div>
<div id="dataset-german-dataset" class="section level4 tabset tabset-fade">
<h4><span class="sub-header">Dataset: German Dataset</span></h4>
<p>Similar to The expreiments run over the English dataset, for the German dataset the experiments show that label Powerset combined with both the random forest and the K-nearest neighbour recorded the best results for the two type of features(TF-IDF and Incidence of terms), compared to the low performance produced by the label powerset method combined with XGBoost. On the other hand, in the contrast to the results of the the English dataset experiments,for the German dataset, we observed higher macro F1 rates over all the nine trained classifiers when we used incidences of terms instead of the TF-IDFs as the instances features.</p>
<div class="figure">
<img src="Figs/German_7000.png" />

</div>
<div class="figure">
<img src="Figs/German_7000_table.png" />

</div>
<p>In the following sections we will display the results of the experiments run on the German dataset in detail for each type of features separately.</p>
</div>
<div id="dataset-german-dataset-feature-tf-idf" class="section level4 tabset tabset-fade">
<h4><span class="sub-sub-header">Dataset: German dataset, Feature: TF-IDF</span></h4>
<p>Extracting TF-IDFs features is computentially expensive. Comparing the low performance of the classifiers to the better performance of the one trained on the incidence of terms, lead us to qualify the terms incidences as more effective features to train the classifiers on. Unlike the English experiments, the label powerset combined with the k-nearest neighbour deliverd slightly higher performance than the classifier combining the label powerset with the random forest.</p>
<div id="label-powerset-2" class="section level5">
<h5>Label Powerset</h5>
<p><img src="Figs/lp_DE_tfidf1.png" /> <img src="Figs/lp_DE_tfidf2.png" /></p>
</div>
<div id="binary-relevance-2" class="section level5">
<h5>Binary Relevance</h5>
<p>The performance of the binary relevance was significantly poor over the three models. <img src="Figs/br_DE_tfidf1.png" /> <img src="Figs/br_DE_tfidf2.png" /></p>
</div>
</div>
<div id="dataset-german-dataset-feature-incidence-of-terms" class="section level4 tabset tabset-fade">
<h4><span class="sub-sub-header">Dataset: German dataset, Feature: Incidence of terms</span></h4>
<p>Based on the better performance deliverd when the classifiers trained on the the incidence of terms, we can consider the incidence of terms as sufficient features instead of the computentially expensive extraction of the TF-IDF features.</p>
<div id="label-powerset-3" class="section level5">
<h5>Label Powerset</h5>
<div class="figure">
<img src="Figs/lp_DE_inc1.png" />

</div>
<div class="figure">
<img src="Figs/lp_DE_inc2.png" />

</div>
</div>
<div id="binary-relevance-3" class="section level5">
<h5>Binary Relevance</h5>
<div class="figure">
<img src="Figs/br_DE_inc1.png" />

</div>
<div class="figure">
<img src="Figs/br_DE_inc2.png" />

</div>
</div>
</div>
</div>
<div id="dataset-with-reduced-number-of-labelsets" class="section level3">
<h3>Dataset with reduced number of labelsets</h3>
<p>Training our classification models on a dataset with such a large number of labels(7000 labels) was a challenging task. we expected that reducing the number of labels would improve the predective capacity of the classifiers. To reduce the large number of labels, we take advangtage of the “scumble” measure provided by the <strong>mldr</strong> package. Scumble measure indicates the concurrence level among frequent and infrequent labels in the same labelsets. we simply removed imbalanced labelsets(i.e imbalanced labelsets are labelsets with frequent and infrequent labels) and kept only balanced labelsets by filtering only labelsets with lower level of scumble values than the mean scumble value of the dataset with the following command line:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">datasetWithBalancedLabelsets &lt;-<span class="st"> </span>dataset[.SCUMBLE <span class="op">&lt;=</span><span class="st"> </span>dataset<span class="op">$</span>measures<span class="op">$</span>scumble]</code></pre></div>
<div id="dataset-english-dataset-balanced-labelsets" class="section level4">
<h4><span class="sub-header">Dataset: English dataset, balanced labelsets</span></h4>
<p>Generaly, pruning the labelsets through removing imbalanced labelsets improved the performance. Some classifiers scored higher than the highest macro F1 value scored in the case of keeping imbalanced labelsets. Although employing the TF-IDF features yielded in higher macro F1 for classifiers trained on the complete set of labels, in the case of balanced labelsets, classifiers trained on the incidence of terms showed slightly better performance as shown in the following graphs:</p>
<div class="figure">
<img src="Figs/English_reduced.png" />

</div>
<div class="figure">
<img src="Figs/English_reduced_table.png" />

</div>
</div>
<div id="dataset-english-dataset-feature-tf-idf-balanced-labelsets" class="section level4 tabset tabset-fade">
<h4><span class="sub-sub-header"> Dataset: English dataset, Feature: TF-IDF, balanced labelsets</span></h4>
<p>For the TF-IDFs features, most of the classifiers trained on the balanced labelsets maintained almost similar levels of performance compared to the performace with the complete set of labels.</p>
<div id="label-powerset-4" class="section level5">
<h5>Label Powerset</h5>
<div class="figure">
<img src="Figs/lp_EN_tfidf1_red.png" />

</div>
<div class="figure">
<img src="Figs/lp_EN_tfidf2_red.png" />

</div>
</div>
<div id="binary-relevance-4" class="section level5">
<h5>Binary Relevance</h5>
<div class="figure">
<img src="Figs/br_EN_tfidf1_red.png" />

</div>
<div class="figure">
<img src="Figs/br_EN_tfidf2_red.png" />

</div>
</div>
</div>
<div id="dataset-english-dataset-feature-incidence-of-terms-balanced-labelsets" class="section level4 tabset tabset-fade">
<h4><span class="sub-sub-header">Dataset: English dataset, Feature: Incidence of terms, balanced labelsets</span></h4>
<p>For balanced labelsets, we observed better performance for the incidence features for all classifiers compared to the performance with the complete labelsets. The label powerset combined with the random forest classifier outperformed the performance of the same classification model trained on the complete set with the TF-IDF features. We consider incidences of terms as sufficient features even for the English dataset in case of removing imbalanced labelsets.</p>
<div id="label-powerset-5" class="section level5">
<h5>Label Powerset</h5>
<div class="figure">
<img src="Figs/lp_EN_inc1_red.png" />

</div>
<div class="figure">
<img src="Figs/lp_EN_inc2_red.png" />

</div>
</div>
<div id="binary-relevance-5" class="section level5">
<h5>Binary Relevance</h5>
<div class="figure">
<img src="Figs/br_EN_inc1_red.png" />

</div>
<div class="figure">
<img src="Figs/br_EN_inc2_red.png" />

</div>
</div>
</div>
<div id="dataset-german-dataset-balanced-labelsets" class="section level4">
<h4><span class="sub-sub-header">Dataset: German dataset, balanced labelsets</span></h4>
<div class="figure">
<img src="Figs/German_reduced2.png" />

</div>
</div>
<div id="dataset-german-dataset-feature-tf-idf-balanced-labelsets" class="section level4">
<h4><span class="sub-sub-header">Dataset: German dataset, Feature: TF-IDF, balanced labelsets</span></h4>
<p>In the case of removing imbalanced labelsets, the macro F1 value of the label powerset with the random forest classifier increased by around(15%).</p>
<div class="figure">
<img src="Figs/lp_DE_tfidf1_red.png" />

</div>
<div class="figure">
<img src="Figs/lp_DE_tfidf2_red.png" />

</div>
</div>
<div id="dataset-german-dataset-feature-incidence-of-terms-balanced-labelsets" class="section level4">
<h4><span class="sub-sub-header">Dataset: German dataset, Feature: Incidence of terms, balanced labelsets</span></h4>
<div class="figure">
<img src="Figs/lp_DE_rf_inc2_red.png" />

</div>
</div>
</div>
<div id="conclusion" class="section level3">
<h3><span class="sub-header">Conclusion</span></h3>
<blockquote>
<blockquote>
<p>We can conclude for both English and German dataset,the best performance was deliverd by the LP transform combined with the Random Forest classifier after removing imbalanced labelsets through exploiting the scumble measure and trained on the terms incidence in order to gain the highest macro F1 values.</p>
</blockquote>
</blockquote>
<ul>
<li><p>Based on the results of various experiments that we conducted on the different classification models, the best performance was observed for the combination <span class="emphasize">LP transform with Random Forest</span> and <span class="emphasize">LP transform with K-nearest neighbour</span>, compared to BR transform along with the same classifiers .</p></li>
<li><p>As mentioned earlier, LP transform takes into consideration the correlation among the labels. In contrast, the BR transform assumes the labels to be independent and ignores any dependency among labels. We infer that the assumption of the independency of labels does not hold in the case of the EUR-Lex dataset.</p></li>
<li><p>Contrary to the LP method,the BR considers the labels themself. It reduces the number of labels when applying the BR to build an independent classifier for each label, which improves the perfomance of the XGBoost classifier by the model optimization technique the XGB follows.</p></li>
<li><p>In experiments over the BR transformation method, XGBoost performed better than the Random Forest and the k-nearest neighbour classifiers. We surmise, since XGBoost classifier is an ensemble model uses “Boosting” as a deliberate ensemble technique, whereas Random Forest employs “Bagging” as an ensemble technique and KNN encounters the curse of dimensionality.</p></li>
<li><p>CC transform method delivered poor performance across all classication models(k-nearest neighbour, Random Forest, XGBoost).</p></li>
<li><p>XGBoost performed the best with CC, due to the boosting optimization technique the xgboost based on.</p></li>
<li><p>Suprisingly, we found the classifiers trained on the <em>terms incidence</em> features performed better than when trained on the <em>tf-idf</em> features. we think that the occurance of the term in a document was a sufficient distinctive feature of the documents of the legal EUR-Lex dataset.</p></li>
<li><p>With reduced labels we found that the classifiers overall performed better, though not much, for both English and German dataset.</p></li>
</ul>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
