
---
title: "R Notebook"
author: "Suhita"
date: "20 December 2018"
header-includes:
   - \usepackage{bbm}
always_allow_html: yes
output:
  html_document: default
bookdown::html_document2: default
link-citations: yes
csl: biomed-central.csl
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
source("R/requirements.R")

knitr::opts_chunk$set(echo = TRUE)
```

### Overview and motivation

A single text document often has multiple semantic aspects. A single news article related to politics may have aspects related to trade, technology and defense. In the perspective of machine learning, we can interpret the various aspects as multiple class labels of an instance (a document). An introduction of enormous amount of documents in the legal domain, makes it an attractive area for employing automated solutions and therefore a machine learning scenario in step with actual practice. In this project we explore a public multi labelled legal text dataset that has been manually annotated over a decade. It contains laws related to the European Union, including treaties, legislation, case-law and legislative proposals, in 22 different languages. This is popularly known as the EUR-Lex dataset containing about twenty thousand documents, around seven thousand labels and in several European languages. A skewed distribution of multiple labels per document, along with existence of the same data in multiple languages, makes this dataset an interesting proposition. Few publications have used an older version of the dataset which had around four thousand labels. The ones that have used this have reported relatively poor values in the range of 50% (which may be fair, given the high number of labels). To the best of our knowledge there has been no publications for the new dataset having around 7000 labels. 

### Multilable classification
In multi-label classification, each instance in the training set is associated with a set of labels, instead of a single lable, and the task is to predict the *label-sets* of unseen instances, instead of a single label. There is a difference between *multi-class classification* and *multi-label classification*. In multi-class problem the classes or labels are mutually-exclusive, i.e. it makes the assumption that each instance can be assigned to only one label. E.g - an animal can be either a dog or a cat but not both.But in multi-label problem multiple labels may be assigned to an instance. E.g - a movie can belong to a comedy genre as well a detective genre.

### Evaluation meausures
Evaluation measures for a multi-label classification problem needs discussion as it is different from multiclass/binary class problem. In single label classification the commonly used metrics are - accuracy, precision, recall, F1-measure, among others.
In multi-label classification we cannot define misclassification as a hard correct or incorrect, but a prediction comprising subset of actual classes is deemed better than containg none of them.

#### Hamming Loss
Hamming Loss is is an example based measure. It is defined as the fraction of labels that are incorrectly predicted.

\(HL = \frac{1}{N . L} \sum_{l=1}^L\sum_{i=1}^N Y_{i,l} \oplus X_{i,l}\)  
  
  where \oplus denotes exlusive-or, \(X_{i,l} (Y_{i,l})\) stands for boolean that the i-th prediction contains the l-th label. For binary scenario (L=1) equals to (1 - accuracy).
  
#### Micro-average and Macro-average
In order to measure the performance of a multi-class classifier we have to consider the average performance over all classes. There are two different ways of doing this called micro-averaging and macro-averaging.
  A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), whereas a micro-average will aggregate the contributions of all classes to compute the average metric. In a multi-class classification setup, micro-average is preferable if is class imbalance, like our dataset.
  
##### Micro Average
In micro all TPs, TNs, FPs and FNs for each class are summed up and then the average is taken. The micro-average F1 is the harmonic mean of the below two equations.  
  
\(Microaverage Precision Prc^{micro}(D) = \frac{\sum_c TP_c}{\sum_c TP_c + \sum_c FP_c} \)  
  
\(Microaverage Recall Rcl^{micro}(D) =  = \frac{\sum_c TP_c}{\sum_c TP_c + \sum_c FN_c} \) 
  
##### Macro Average
In macro average we take the average of precision and recall of the system on different sets. It is used when we want to know how the algorithm performs overall across different subset of data.  

\(Macrooaverage Precision Prc^{macro}(D) = \frac{\sum_c Prc(D,c)}{|C|} \)  
  
\(Microaverage Recall Rcl^{macro}(D) = \frac{\sum_c Rcl(D,c)}{|C|} \) 



### work on this dataset

### Background and motivation

A single text document often has multiple semantic aspects. A single news article related to politics may have aspects related to trade, technology and defense. In the perspective of machine learning, we can interpret the various aspects as multiple class labels of an instance (a document). In this project we explore a public **multi labelled legal** text dataset that has been manually annotated over a decade. It contains laws related to the European Union, including treaties, legislation, case-law and legislative proposals, in 22 different languages. This is popularly known as the _**EUR-Lex**_ dataset containing about twenty thousand documents and seven thousand labels. A skewed distribution of multiple labels per document, along with existence of the same data in multiple languages, makes this data set an interesting proposition. Few publications have used this dataset; the ones that have used this have reported relatively poor values in the range of 50% (which may be fair, given the high number of labels).

### Project objectives
In this project we first perform a statistical exploratory analysis of the dataset. Secondly, we plan to experiment the performance of various state-of-the-art classifiers on this dataset. For a better understanding of this classification task, we study the evaluation measures for the multilabel scenario.
In the process, we try to answer the following research questions:  
  
* How well the classifiers perform over Eur-Lex dataset for two languages (English and Deutsch).  
* How the classifiers' performance changes with different features- one with term frequency–inverse document frequency(tf-idf), another with term incidence.  
* How the classifiers' performance changes when the number of labels is reduced


### Name of dataset:
European Union law documents (EUR-Lex).
The [data](https://ec.europa.eu/jrc/en/language-technologies/jrc-eurovoc-indexer#Download%20JEX) is located inside the software distributed by European Union.



### Design overview (algorithms and methods we plan to use)
* Statistical exploration  :
     + Basic exploration  - distribution  of attributes/labels
     + Multi-label specific exploration- labelset distribution, relationship among labels, and relationship between attributes and labels/labelsets
* Pre-processing :
     + Exclude stop words, perform stemming or lemmatization.
     + Extract features - term frequency–inverse document frequency(tf-idf) and term incidence.
     + Generate the MLD [@Gibaja:2015:TML:2737799.2716262] data format, which is needed for multi label data exploration and classification using _mldr_[@charte2015working] and _utiml_[@rivolliutiml] packages .
* Apply (atleast) the following classifiers over the preprocessed dataset.
     + IBk (Nearest Neighbour)
     + RandomForest
     + SVM
* The following evaluation measures cab be used to assess the multilabel predictive performance:
     + Accuracy
     + Hamming Loss
     + Average Precision and Recall
     + Coverage
     + Ranking Loss
* Compare the accuracies of the state-of-the-art classifiers on this dataset, for two languages.




#### References
