
---
title: "R Notebook"
author: "Suhita"
date: "20 December 2018"
header-includes:
   - \usepackage{bbm}
always_allow_html: yes
output:
  html_document: 
    toc: true
    toc_depth: 4
    toc_float: 
      collapsed: false
      smooth_scroll: false
    theme: united
    highlight: haddock
    
bookdown::html_document2: default
link-citations: yes
csl: biomed-central.csl
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
source("R/requirements.R")

knitr::opts_chunk$set(echo = TRUE)
```

# Overview and motivation  {.tabset  .tabset-fade}

Most of the classification algorithms deal with datasets which have a set of input features and only *one* output class. However, in reality the problem might be different from a typical binary or multiclass classification. A single text document often has multiple semantic aspects. A single news article related to politics may have aspects related to trade, technology and defense. Therefore, often a document needs to be tagged to multiple labels/categories, instead of a single category. An introduction of enormous amount of documents belonging to multiple categories in the legal domain, makes it an attractive area for employing automated solutions.  
In this project we explore a public multi labelled legal text dataset that has been manually annotated over a decade. It contains laws related to the European Union, including treaties, legislation, case-law and legislative proposals, in 22 different languages. This is popularly known as the **EUR-Lex** dataset containing about twenty thousand documents, around seven thousand labels and in several European languages. A skewed distribution of multiple labels per document, along with existence of the same data in multiple languages, makes this dataset an interesting proposition. Few publications have used an older version of the dataset which had around four thousand labels. The ones that have used this have reported relatively poor values in the range of 50% (which may be fair, given the high number of labels). To the best of our knowledge there has been no publications for the new dataset having around 7000 labels. 

# Multilable v/s Multiclass classification
In multi-label classification, each instance in the training set is associated with a set of labels, instead of a single lable, and the task is to predict the *label-sets* of unseen instances, instead of a single label. There is a difference between *multi-class classification* and *multi-label classification*. In multi-class problem the classes or labels are mutually-exclusive, i.e. it makes the assumption that each instance can be assigned to only one label. E.g - an animal can be either a dog or a cat but not both. But in multi-label problem multiple labels may be assigned to an instance. E.g - a movie can belong to a comedy genre as well a detective genre.
![Multilable v/s Multiclass classification](Figs/multiclass-label.png)



# Working with multilabel datasets

## Multilabel dataset traits
Multi-label datasets (MLD) are different from binary/multi-class ones as they have multiple class per instance instead of one. Therefore each instance in MLD has a set of features(attributes) and a set of labels (labelsets). It is not rare in MLDs that there are more labels than features. Our dataset has around 7000 labels and lesser number of attributes. Our dataset like most MLDs is very imbalanced. The labels in an MLD can be correlated or not. Moreover, frequent labels and rare labels can appear together in the same instances. The figure below outlines the measures. Some of the important measures have been explained below.
  

![measures for MLD](Figs/measures.png)

Basic measures : { .subheader}  

The most basic information that can be obtained from an MLD is the number of instances, attributes
and labels. Each instance has an associated labelset, whose length (number of active labels) can be in the range
{0..|L|}.

**Label related measures :**  

- Card : The average number of active labels per instance is the most basic measure of any MLD, usually
known as Card (standing for cardinality).

- Dens : Dividing * Card * by the number of labels results in a dimension-less measure, known as Dens (standing for label density).

- IRLb : Most multilabel datasets are imbalanced, meaning that some of the labels are very frequent whereas
others are quite rare. The level of imbalance of a determinate label can be measured by the imbalance ratio,

- UniqLabelsets : The number of different labelsets, as well as the amount of them being unique labelsets (appearing only
once in D), give us a glimpse on how sparsely the labels are distributed.

- SCUMBLE : is used to assess the concurrence level among frequent and infrequent labels.

## Multilabel classification

There are two possibilities to deal with multi-label classification:
- **Algorithm adaptation:** Modify existing algorithms taking into account the multilabel nature of the samples, for instance hosting more than one class in the leaves of a tree instead of only one.
- **Problem transformation:** Transforming the original data to make it suitable to existing traditional classification algorithms and combining the obtained predictions to build the labelsets given as output result.  

There are several transformation methods in literature. Three have been defined and used for our case study.  

- **Binary Relevance (BR):** Introduced by [8] as an adaptation of OVA (one-vs-all ) to the multilabel scenario, this method transforms the original multilabel dataset into several binary datasets. Here an ensemble of binary classifiers is trained, one for each class. Each classifier predicts either the membership or the non-membership of one class. A union of all predicted classes is taken as the multi-label output. The approach is popular because it is easy to implement, but it ignores the possible correlations between class labels.  

- **Label Powerset (LP):** Introduced by [1], this method transforms the multilabel dataset into a multiclass dataset by using the labelset of each instance as class identifier. This approach does take possible correlations between class labels into account. The downside of the method is it has a high computational complexity and when the number of classes increases the number of distinct label combinations can grow exponentially. This easily leads to combinatorial explosion and thus computational infeasibility. The method is called *Label Powerset* because it considers each member of the power set of labels in the training set as a single label.

- **Classifier Chains (CC):** Introduced in [15] , this method comprises a chain of binary classifiers \(C_0, C_1, . . . , C_m \) is constructed, where a classifier \(C_i\) uses the predictions of all the classifier \(C_j\) , where j < i. This way the method can take into account label correlations. The total number of classifiers needed for this approach is equal to the number of classes, but the training of the classifiers is more involved. 


## Evaluation metric

Evaluation measures for a multi-label classification problem needs discussion as it is different from multiclass/binary class problem. In single label classification the commonly used metrics are - accuracy, precision, recall, F1-measure, among others.
In multi-label classification we cannot define misclassification as a hard correct or incorrect, but a prediction comprising subset of actual classes is deemed better than containg none of them.

### Hamming Loss
Hamming Loss is is an example based measure. It is defined as the fraction of labels that are incorrectly predicted.

\(HL = \frac{1}{N . L} \sum_{l=1}^L\sum_{i=1}^N Y_{i,l} \oplus X_{i,l}\)  
  
  where \oplus denotes exlusive-or, \(X_{i,l} (Y_{i,l})\) stands for boolean that the i-th prediction contains the l-th label. For binary scenario (L=1) equals to (1 - accuracy).
  
### Micro-average and Macro-average
In order to measure the performance of a multi-class classifier we have to consider the average performance over all classes. There are two different ways of doing this called micro-averaging and macro-averaging.
  A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), whereas a micro-average will aggregate the contributions of all classes to compute the average metric. In a multi-class classification setup, micro-average is preferable if is class imbalance, like our dataset.
  
#### Micro Average
In micro all TPs, TNs, FPs and FNs for each class are summed up and then the average is taken. The micro-average F1 is the harmonic mean of the below two equations.  
  
\(Microaverage Precision Prc^{micro}(D) = \frac{\sum_c TP_c}{\sum_c TP_c + \sum_c FP_c} \)  
  
\(Microaverage Recall Rcl^{micro}(D) =  = \frac{\sum_c TP_c}{\sum_c TP_c + \sum_c FN_c} \) 
  
#### Macro Average
In macro average we take the average of precision and recall of the system on different sets. It is used when we want to know how the algorithm performs overall across different subset of data.  

\(Macrooaverage Precision Prc^{macro}(D) = \frac{\sum_c Prc(D,c)}{|C|} \)  
  
\(Microaverage Recall Rcl^{macro}(D) = \frac{\sum_c Rcl(D,c)}{|C|} \) 


#### References
