---
title: "My Notebook"
output: html_notebook
---




```{r init, message=FALSE, warning=FALSE, include=TRUE}

## Required packages
source("../scripts/requirements.R")

## Functions needed
source("../scripts/preprocessing_utility.R")

## Project settings
source("../configuration/en.R")

```

The process notebook comprises steps for language `r lang`:

- Text preprocessing
- Exploratory analysis over legal text
- Preprocessing for mldr
- Exploratory analysis over MLD dataset
- Classification using LP, BR, and CC
- Classification using reduced labels using LP and BR

The following code will load and preprocess the text.

```{r load_preprocess_text, include=TRUE, eval=TRUE}

connection <- fileName  %>% file(open = "r")
raw_text_char <- connection %>% readLines(encoding = "UTF-8")
close.connection(connection)

sample_no <- ifelse((exists("doc_number") && doc_number>100), doc_number, length(raw_text_char))

text_content_list <- raw_text_char[seq (2, sample_no, 2)] #every even line contains text

text_content_list[[1]]

text_content_list <- get_clean_content(text_content_list) #get preprocessed text

text_content_list[[1]]
```

We need to preprocess labels, as label names are in different file. The dataset- *acquis.cf* file just contains label-ids. Also the label names contains certain characters like space which is inconvenient to generate MLD datasets. The following code generates clean label name for all documents.

```{r get_clean_label_lst, include=TRUE, eval=TRUE}
library(XML)

class_labels_list <-
  raw_text_char[seq (1, sample_no, 2)] %>%
  strsplit("#") %>%
  sapply("[[", 1) %>%
  trimws() %>%
  get_label_name_list()

class_labels_list[[1]]
```

After generating a cleaner version of text and labels we can take a deep dive into the first part of our data exploration.

We generate a dataframe such that each row contains one document's text and one label.

```{r exploration_df, include=TRUE, eval=TRUE}
text <- character()
label <- character()
for (index in 1:length(text_content_list)) {
  temp_labelset <- unlist(class_labels_list[[index]])
  for (label_index in 1:length(temp_labelset))
  {
    text <- append(text, text_content_list[[index]])
  label <- append(label, temp_labelset[[label_index]])
  }
}

text_df <- as.data.frame(cbind(text, label), stringsAsFactors = FALSE)

text_df[1:4,]
```

We start exploration with wordcloud, whichis a simple yet informative way to understand textual data and perform analysis.

```{r wordcloud_all, include=TRUE, eval=TRUE}
library(tidytext)

#generate tokens and count of each word from text 
tokens <- text_df %>%
  unnest_tokens(word, text) %>%
  dplyr::count( word, sort = TRUE) %>%
  ungroup()

library(wordcloud)

wordcloud(words = tokens$word, freq = tokens$n, min.freq = 1, max.words=50, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"),scale=c(3.5,0.25))
```

We see that the word *state* stands out among other terms, and it seems the laws/treaties are related to state than a country. After getting some insight from the wordcloud we examine it further. It will be interesting to see which terms are common for different category of legal text. To get that, we need to count words for each category.

```{r tf, include=TRUE, eval=TRUE}
tokens_by_label <- text_df %>%
  unnest_tokens(word, text) %>% #generate tokens
  dplyr::count(label, word, sort = TRUE) %>% #counts words for each category
  ungroup()
  
  total_words <- tokens_by_label %>%
    group_by(label) %>%
    summarize(total = sum(n))
  
  tokens_by_label <- left_join(tokens_by_label, total_words)
  
  tokens_by_label 
```

There is one row in the data frame *tokens_by_label* for each word-label combination. *n* is the number of times that word is used in that legal text category/label and *total* is the total words in that category. Let us have a look at the distribution of (n/total) for each document category- the number of times a word appears in a label divided by the total number of terms in that category. Our dataset has around 6797 labels , and it will be difficult to analyse over all categories! Therfore, we will consider some of the labels for our analysis.

```{r tf_graph, include=TRUE, eval=TRUE, warning=FALSE}
library(ggplot2)

selected_labels <- unique(tokens_by_label$label)[1:8]

tokens_by_label %>% filter(
  label %in% selected_labels) %>%
  ggplot(aes(n / total, fill = label)) +
  geom_histogram(show.legend = FALSE, bins=5) +
  xlim(NA, 0.0009) +
  facet_wrap( ~ label, ncol = 2, scales = "free_y")
```

Some of the plots portray long tails on the right, and we can use the term frequency dataframe to plot term frequency and examine Zipf's law.

>> Zipf's law states that the frequency that a word appears is inversely proportional to its rank.

```{r zipf, include=TRUE, eval=TRUE}


freq_by_rank <- tokens_by_label %>%
  group_by(label) %>%
  mutate(rank = row_number(),
  `term frequency` = n / total)

freq_by_rank %>%
  ggplot(aes(rank, `term frequency`, color = label)) +
  geom_line(size = 1.1,
  alpha = 0.8,
  show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()

```

The above plot is in log-log coordinates. We can see that text in all the categories in the corpus are almost similar to each other, and that the relationship between rank and frequency does have negative slope, implying that they follow Zipf's Law.

It would be interesting to find out top 10 words for each category, which will give us an idea what each category of law document deals with. To find the most importants words it would make sense to use tf-idf rather than term frequency, as tf-idf finds the important words for the content of each document category by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents.  

```{r topk_mono, include=TRUE, eval=TRUE}

tokens_by_label <- bind_tf_idf(tokens_by_label, word, label, n) #enerates tf-idf and binds to dataframe
tokens_by_label
```

```{r topk_plot, include=TRUE, eval=TRUE}

tokens_by_label %>%
  filter(
  label %in% selected_labels[1:4] ) %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(word, levels = rev(unique(word)))) %>%
  group_by(label) %>%
  top_n(10) %>%
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = label)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = NULL) +
  facet_wrap( ~ label, ncol = 2, scales = "free") +
  coord_flip()
```

We can see common terms appearing for labels *seed_* ,*fodder_plant_* and *marketing_standard_*. This implies many documents for marketing standard deals with seed and fodder plant. *approximation_of_laws* also has the term plant, which means it also has documents related to plants.  
Sometimes bigrams are more meaningful than single words, and it would be interesting to see how the top 10 words changes for bigrams.

We generate the tokens in similar way, except we use we used *token = "ngrams"* and *n=2* in the method *unnest_tokens.*

```{r topk_b_plot, include=TRUE, eval=TRUE}

bigram_tokens <- text_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  dplyr::count(label, bigram, sort = TRUE) %>%
  ungroup()
  
  total_words <- bigram_tokens %>%
  group_by(label) %>%
  summarize(total = sum(n))
  
  bigram_tokens <- left_join(bigram_tokens, total_words)
  bigram_tokens <- bind_tf_idf(bigram_tokens, bigram, label, n)
  
  bigram_tokens %>%
  filter(
  label %in% selected_labels[1:4]) %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
  group_by(label) %>%
  top_n(10) %>%
  ungroup %>%
  ggplot(aes(bigram, tf_idf, fill = label)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = NULL) +
  facet_wrap( ~ label, ncol = 2, scales = "free") +
  coord_flip()
```

We can see similar observation as seen in top-k words.

Another way to view word connections is to treat them as a network, similar to a social network. Word networks show term association and cohesion. In a network graph, the circles are called nodes and represent individual terms, while the lines connecting the circles are called edges and represent the connections between the terms. We can see some interesting word association in the Fig below.

```{r worda, include=TRUE, eval=TRUE}
library(tidyr)
library(igraph)
library(ggraph)

(
  bigram_graph <- bigram_tokens %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  dplyr::count(word1, word2, sort = TRUE) %>%
  unite("bigram", c(word1, word2), sep = " ") %>%
  top_n(100) %>%
  graph_from_data_frame()
  )
  
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1, repel=TRUE) +
  theme_void() 
```
Preprocessing for mldr-

To perform the exploratory analysis over the MLD dataset traits we need to generate the dataset in the format which can read by **mldr** package. First we generate the corpus and then document term matrix each for tf-idf and incidence.

```{r dtms, include=TRUE, eval=TRUE}
#corpus of text
text_corpus <- VCorpus(VectorSource(text_content_list))

#corpus of labels just like text corpus
label_corpus <- VCorpus(VectorSource(class_labels_list))
#dtm of labels
dtm_labels <- DocumentTermMatrix(label_corpus, control=list(weight=weightTfIdf))

# dtm matrix for tf-idf. considering words having atleast 3 letters
dtm_tfidf <- text_corpus %>%
  DocumentTermMatrix(control = list(
  wordLengths = c(3, Inf),
  weighting = function(x)
  weightTfIdf(x, normalize = FALSE) ,
  stopwords = TRUE
  ))  %>%
  removeSparseTerms(0.99) # remove sparse terms, so that sparsity is maximum 99%
  
# column bind text and labels to generate tfidf ARFF file
dtm_tfidf <- cbind(dtm_tfidf, dtm_labels)

# gets unique words- needed for incidence matrix
  uniqueWords <- function(text) {
    return(paste(unique(strsplit(text, " ")[[1]]), collapse = ' '))
  }

# dtm matrix for incidence. considering words having atleast 3 letters
dtm_incidence <-  text_corpus %>%
    tm_map(content_transformer(uniqueWords)) %>%
    DocumentTermMatrix(control = list(
    wordLengths = c(3, Inf),
    weight = weightBin ,
    stopwords = TRUE
    )) %>%
    removeSparseTerms(0.99)

# column bind text and labels to generate incidence ARFF file
dtm_incidence <- cbind(dtm_incidence,dtm_labels)
```

After the document term matrices have been generated we need to generate the ARFFs and the xmls

```{r arff_gen, include=TRUE, eval=TRUE}

#generate ARFF for mldr
generate_ARFF(dtm_incidence, paste(incFileName,".arff",sep="")) #generate arff for incidence
generate_ARFF(dtm_tfidf, paste(tfidfFileName,".arff",sep="")) #generate arff for tfidf

#generate XML for mldr
label_names <-
xmlParse(labelFile) %>% xpathApply("//LIBELLE", xmlValue) %>% get_clean_label()
xml_root = newXMLNode("labels")

for (i in 1:length(label_names)) {
  newXMLNode("label", attrs = c(name = label_names[i]), parent = xml_root)
}

saveXML(xml_root,file=paste(incFileName,".xml", sep="")) #xml for incidence
saveXML(xml_root,file=paste(tfidfFileName,".xml", sep="")) #xml for tfidf
```

After mldr compliant datasets are generated, they are loaded. Inspection of one of them (incidence) will be fine for further analysis, as they have same features and labels, but different values. tf-idf features will have a positive real number and incidence feature values will be 0 or 1 (incidence of terms).

```{r load_mldr, include=TRUE, cached=TRUE}
eurlex <- mldr(paste(tfidfFileName))

summary(eurlex)
```

Labels' information in the MLD, including the number of times they appear, their IRLbl and SCUMBLE measures, can be retrieved by using the *labels* member of the *mldr* class.

```{r mldr_label, include=TRUE, cached=TRUE}
#inspect first 20 labels
head(eurlex$labels, 20)

```
The concurrence plot explores interactions among labels. 
We can choose the labels from the mldr attribute *eurlex\$labels*. The plots of mldr use *eurlex\$attributes*, and they contain all features and labels. The *eurlex\$attributes* first start with features and then followed by labels.

```{r mldr_attrib, include=TRUE, cached=TRUE }
head(eurlex$attributes)

tail(eurlex$attributes)

#selecting first 20 labels/categories
plot_labels <- eurlex$labels$index[1:40]

names(eurlex$attributes)[plot_labels]


```

The concurrence plot explores interactions among labels. This plot has a circular shape, with the circumference partitioned into many disjoint arcs representing labels. Each arc has length proportional to the number of instances where the label is present. These arcs are in turn divided into bands that join two of them, showing the relation between the corresponding labels. The width of each band is proportional to the number of instances in which both labels appear simultaneously. Since drawing interactions among all the labels can produce a confusing result, we have plotted for some labels.
We can plot the concurrence plot using the plot() using type "LC"

```{r plot_lc, include=TRUE, eval=TRUE }
plot(eurlex, type="LC", labelIndices=plot_labels, title="Concurrence plot")

```

We can see a thick blue band joining cattle_ and ani_ase(animal_disease), which means there are instances common to labels animal_disease_ and cattle_. There are no bands connecting common_agricultural_policy_(com_icy) and animal_disease (ani_ase), which means there are no common instances between them.

The label histogram relates labels and instances in a way that shows how well-represented labels are in general. The X axis is assigned to the number of instances and the Y axis to the amount of labels. In our plot the data is concentrated on the left side of the plot, which means that a great number of labels are appearing in very few instances.

```{r plot_lh, include=TRUE, eval=TRUE }
plot(eurlex, type='LH',col = brewer.pal(11, 'Spectral'), title='Eurlex')
```

We can see most of the instances have very less labels considering the total number of labels is around 7000.

The labelset histogram is similar to LH plot but, instead of representing the number of instances in which each label appears, it shows the amount of labelsets.

```{r plot_lsh, include=TRUE, eval=TRUE }
plot(eurlex, type='LSH',col = brewer.pal(11, 'Spectral'), title='Eurlex', ylim=c(0,100))
```

The cardinality histogram represents the amount of labels instances have in general. Since data has accumulated on the left side of the plot it means that very less instances have a notable amount of labels.

```{r plot_ch, include=TRUE, eval=TRUE }
plot(eurlex, type='CH',col = brewer.pal(11, 'Spectral'), title='Eurlex')
```


# Classification

To perform classification we generate multiple batches of datasets (arff and xml), as mldr is very slow in loading large arff files and performing classification.

```{r generate_dataset_clsf, message=TRUE, include=TRUE,eval=TRUE }

## Generate dataset for classification
source("../scripts/generate_dataset.R")
```

Classification for tf-idf dataset using Binary Relevance Transform (BR)

```{r br_tf, message=TRUE, include=TRUE,eval=TRUE }
source("../scripts/classification_utility.R")

knn_tfidf_br_results <- classify(tfidfFileName,"br","KNN", FALSE)
rf_tfidf_br_results <- classify(tfidfFileName,"br","RF", FALSE)
xgb_tfidf_br_results <- classify(tfidfFileName,"br","XGB", FALSE)
```
After we get results for each batch we try to get the average performance over the batches.

```{r br_tfidf_plot, message=TRUE, include=TRUE,eval=TRUE }
knn_tfidf_br_results_mean <- rowMeans(knn_tfidf_br_results)
rf_tfidf_br_results_mean <- rowMeans(rf_tfidf_br_results)
xgb_tfidf_br_results_mean <- rowMeans(xgb_tfidf_br_results)

if(exists("performance")) rm(performance)

performance=as.data.frame(rbind( knn_tfidf_br_results_mean, rf_tfidf_br_results_mean, xgb_tfidf_br_results_mean))
performance[["model"]] <- c("knn",'rf','xgb')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0)) 
```

We can observe Random Forest clearly performs best among all, with respect to all metrics. XGB performs slightly better than KNN with respect to most of the metrics.


Classification for tf-idf dataset using LabelPowerset Transform (LP)

```{r lp_tf, message=TRUE, include=TRUE,eval=TRUE }
knn_tfidf_lp_results <- classify(tfidfFileName,"lp","KNN", FALSE)
rf_tfidf_lp_results <- classify(tfidfFileName,"lp","RF", FALSE)
xgb_tfidf_lp_results <- classify(tfidfFileName,"lp","XGB", FALSE)
```
We similarly plot for LP

```{r lp_tfidf_plot, message=TRUE, include=TRUE,eval=TRUE }
knn_tfidf_lp_results_mean <- rowMeans(knn_tfidf_lp_results)
rf_tfidf_lp_results_mean <- rowMeans(rf_tfidf_lp_results)
xgb_tfidf_lp_results_mean <- rowMeans(xgb_tfidf_lp_results)
rm(performance)

performance=as.data.frame(rbind( knn_tfidf_lp_results_mean, rf_tfidf_lp_results_mean, xgb_tfidf_lp_results_mean))
performance[["model"]] <- c("knn",'rf','xgb')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0))

```

Here also we observe Random Forest performs best among all and XGB worst, with respect to all metrics.

Classification for tf-idf dataset using Classifier Chain Transform (CC)

```{r cc_tfidf, message=TRUE, include=TRUE,eval=TRUE }
knn_tfidf_cc_results <- classify(tfidfFileName,"cc","KNN", FALSE)
rf_tfidf_cc_results <- classify(tfidfFileName,"cc","RF", FALSE)
xgb_tfidf_cc_results <- classify(tfidfFileName,"cc","XGB", FALSE)

```

We also plot for CC

```{r cc_tfidf_plot, message=TRUE, include=TRUE,eval=TRUE }
knn_tfidf_cc_results_mean <- rowMeans(knn_tfidf_cc_results)
rf_tfidf_cc_results_mean <- rowMeans(rf_tfidf_cc_results)
xgb_tfidf_cc_results_mean <- rowMeans(xgb_tfidf_cc_results)
rm(performance)

performance <- as.data.frame(rbind( knn_tfidf_cc_results_mean, rf_tfidf_cc_results_mean, xgb_tfidf_cc_results_mean))
performance[["model"]] <- c("knn",'rf','xgb')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0))

```
Here also we observe Random Forest perfoms best among all, and KNN has performed similarly with respect to some metrics(micro-F1, macro-F1 and hamming-loss).

Now let's see which flavour of MLD algorithms performs best. Considering only random forest classifiers for BR, LP and CC, as it is the clear winner in all the cases.


```{r mld_tfidf_plot, message=TRUE, include=TRUE,eval=TRUE }
rm(performance)

performance <- as.data.frame(rbind( rf_tfidf_br_results_mean, rf_tfidf_lp_results_mean, rf_tfidf_cc_results_mean))
performance[["model"]] <- c("br",'lp','cc')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

library(gridExtra)

g1 <- ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0))

g2 <- ggplot(perf_metric)+geom_tile(aes(x=model,y=metrics,fill=value),color="black")+geom_text(aes(x=model,y=metrics,label=round(value,3)),color="black")+scale_fill_distiller(palette = "Spectral")

grid.arrange(g1, g2, nrow = 1)

```

We observe LP is the clear winner with respect to all metrics except micro-precision. Since LP is the winner it implies 

Classification for incidence dataset done similarly by using BR, LP and CC

```{r inc_all , message=TRUE, include=TRUE,eval=TRUE }
knn_inc_br_results <- classify(incFileName,"br","KNN", FALSE)
rf_inc_br_results <- classify(incFileName,"br","RF", FALSE)
xgb_inc_br_results <- classify(incFileName,"br","XGB", FALSE)

knn_inc_lp_results <- classify(incFileName,"lp","KNN", FALSE)
rf_inc_lp_results <- classify(incFileName,"lp","RF", FALSE)
xgb_inc_lp_results <- classify(incFileName,"lp","XGB", FALSE)

knn_inc_cc_results <- classify(incFileName,"cc","KNN", FALSE)
rf_inc_cc_results <- classify(incFileName,"cc","RF", FALSE)
xgb_inc_cc_results <- classify(incFileName,"cc","XGB", FALSE)
```

```{r br_inc_plot, message=TRUE, include=TRUE,eval=TRUE }
knn_inc_br_results_mean <- rowMeans(knn_inc_br_results)
rf_inc_br_results_mean <- rowMeans(rf_inc_br_results)
xgb_inc_br_results_mean <- rowMeans(xgb_inc_br_results)
rm(performance)

performance <- as.data.frame(rbind( knn_inc_br_results_mean, rf_inc_br_results_mean, xgb_inc_br_results_mean))
performance[["model"]] <- c("knn",'rf','xgb')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0))

```

```{r br_lp_inc_plot, message=TRUE, include=TRUE,eval=TRUE }
knn_inc_lp_results_mean <- rowMeans(knn_inc_lp_results)
rf_inc_lp_results_mean <- rowMeans(rf_inc_lp_results)
xgb_inc_lp_results_mean <- rowMeans(xgb_inc_lp_results)
rm(performance)

performance <- as.data.frame(rbind( knn_inc_lp_results_mean, rf_inc_lp_results_mean, xgb_inc_lp_results_mean))
performance[["model"]] <- c("knn",'rf','xgb')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0))

```

 
```{r cc_inc_plot, message=TRUE, include=TRUE,eval=TRUE }
knn_inc_cc_results_mean <- rowMeans(knn_inc_cc_results)
rf_inc_cc_results_mean <- rowMeans(rf_inc_cc_results)
xgb_inc_cc_results_mean <- rowMeans(xgb_inc_cc_results)
rm(performance)

performance=as.data.frame(rbind( knn_inc_cc_results_mean, rf_inc_cc_results_mean, xgb_inc_cc_results_mean))
performance[["model"]] <- c("knn",'rf','xgb')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0))

```

In all the plots for incidence also we observe Random Forest is the winner. Let's see which flavour of MLD classification performs best.  Here also we only consider Random Forest.

```{r mld_inc_plot, message=TRUE, include=TRUE,eval=TRUE }
rm(performance)

performance <- as.data.frame(rbind( rf_inc_br_results_mean, rf_inc_lp_results_mean, rf_inc_cc_results_mean))
performance[["model"]] <- c("br",'lp','cc')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

library(gridExtra)

g1 <- ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0))

g2 <- ggplot(perf_metric)+geom_tile(aes(x=model,y=metrics,fill=value),color="black")+geom_text(aes(x=model,y=metrics,label=round(value,3)),color="black")+scale_fill_distiller(palette = "Spectral")

grid.arrange(g1, g2, nrow = 1, ncol=2)

```

Here also LP performs the best, except for *Micro-Precision*.

Now let us see which features proved to be better. We consider only Random Forest here also for the same before mentioned reason.
```{r mld_inc_tfidf_plot, message=TRUE, include=TRUE,eval=TRUE }
rm(performance)

performance <- as.data.frame(rbind( rf_tfidf_lp_results_mean, rf_inc_lp_results_mean))
performance[["model"]] <- c('tf-idf','incidence')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

library(gridExtra)

g1 <- ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0))

g2 <- ggplot(perf_metric)+geom_tile(aes(x=model,y=metrics,fill=value),color="black")+geom_text(aes(x=model,y=metrics,label=round(value,3)),color="black")+scale_fill_distiller(palette = "Spectral")

grid.arrange(g1, g2, nrow = 1, ncol=2)

```

Both of them performed similar and it is difficult to say which one is better than the other.


Classification for tf-idf and incidence dataset done similarly by using BR and LP, but using reduced labels. By reducing labels we mean the majority labels are disabled on instances with highly imbalanced labels. We will try to answer the research question whether classification accuracy improves with reduced labels. We are considering only tf-idf dataset as performance of the classifiers was comparable.

```{r cls_red_labels , message=TRUE, include=TRUE,eval=TRUE }
knn_tfidf_br_red_results <- classify(tfidfFileName,"br","KNN", TRUE)
rf_tfidf_br_red_results <- classify(tfidfFileName,"br","RF", TRUE)
xgb_tfidf_br_red_results <- classify(tfidfFileName,"br","XGB", TRUE)

knn_tfidf_lp_red_results <- classify(tfidfFileName,"lp","KNN", TRUE)
rf_tfidf_lp_red_results <- classify(tfidfFileName,"lp","RF", TRUE)
xgb_tfidf_lp_red_results <- classify(tfidfFileName,"lp","XGB", TRUE)

knn_tfidf_cc_red_results <- classify(tfidfFileName,"cc","KNN", TRUE)
rf_tfidf_cc_red_results <- classify(tfidfFileName,"cc","RF", TRUE)
xgb_tfidf_cc_red_results <- classify(tfidfFileName,"cc","XGB", TRUE)

knn_tfidf_br_red_results_mean <- rowMeans(knn_tfidf_br_red_results,na.rm = TRUE)
rf_tfidf_br_red_results_mean <- rowMeans(rf_tfidf_br_red_results,na.rm = TRUE)
xgb_tfidf_br_red_results_mean <- rowMeans(xgb_tfidf_br_red_results,na.rm = TRUE)

knn_tfidf_lp_red_results_mean <- rowMeans(knn_tfidf_lp_red_results,na.rm = TRUE)
rf_tfidf_lp_red_results_mean <- rowMeans(rf_tfidf_lp_red_results,na.rm = TRUE)
xgb_tfidf_lp_red_results_mean <- rowMeans(xgb_tfidf_lp_red_results,na.rm = TRUE)

knn_tfidf_lp_red_results_mean <- rowMeans(knn_tfidf_lp_red_results,na.rm = TRUE)
rf_tfidf_lp_red_results_mean <- rowMeans(rf_tfidf_lp_red_results,na.rm = TRUE)
xgb_tfidf_lp_red_results_mean <- rowMeans(xgb_tfidf_lp_red_results,na.rm = TRUE)

knn_tfidf_cc_red_results_mean <- rowMeans(knn_tfidf_cc_red_results,na.rm = TRUE)
rf_tfidf_cc_red_results_mean <- rowMeans(rf_tfidf_cc_red_results,na.rm = TRUE)
xgb_tfidf_cc_red_results_mean <- rowMeans(xgb_tfidf_cc_red_results,na.rm = TRUE)

knn_tfidf_cc_red_results_mean <- rowMeans(knn_inc_cc_results,na.rm = TRUE)
rf_tfidf_cc_red_results_mean <- rowMeans(rf_inc_cc_results,na.rm = TRUE)
xgb_tfidf_cc_red_results_mean <- rowMeans(xgb_inc_cc_results,na.rm = TRUE)



```

```{r mld_red_label_plot, message=TRUE, include=TRUE,eval=TRUE }
knn_tfidf_lp_red_results_mean <- rowMeans(knn_tfidf_lp_red_results,na.rm = TRUE)
rf_tfidf_lp_red_results_mean <- rowMeans(rf_tfidf_lp_red_results,na.rm = TRUE)
xgb_tfidf_lp_red_results_mean <- rowMeans(xgb_tfidf_lp_red_results,na.rm = TRUE)

knn_tfidf_cc_red_results_mean <- rowMeans(knn_tfidf_cc_red_results,na.rm = TRUE)
rf_tfidf_cc_red_results_mean <- rowMeans(rf_tfidf_cc_red_results,na.rm = TRUE)
xgb_tfidf_cc_red_results_mean <- rowMeans(xgb_tfidf_cc_red_results,na.rm = TRUE)

knn_tfidf_cc_red_results_mean <- rowMeans(knn_inc_cc_results,na.rm = TRUE)
rf_tfidf_cc_red_results_mean <- rowMeans(rf_inc_cc_results,na.rm = TRUE)
xgb_tfidf_cc_red_results_mean <- rowMeans(xgb_inc_cc_results,na.rm = TRUE)

if(exists("performance_br"))rm(performance_br)

performance_br <- as.data.frame(rbind( knn_tfidf_br_red_results_mean, rf_tfidf_br_red_results_mean,xgb_tfidf_br_red_results_mean))
performance_br[["model"]] <- c('knn','rf','xgb')
performance_br <- performance_br[names(performance_br) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

if(exists("performance_lp"))rm(performance_lp)

performance_lp <- as.data.frame(rbind( knn_tfidf_lp_red_results_mean, rf_tfidf_lp_red_results_mean,xgb_tfidf_lp_red_results_mean))
performance_lp[["model"]] <- c('knn','rf','xgb')
performance_lp <- performance_lp[names(performance_lp) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

if(exists("performance_cc"))rm(performance_cc)

performance_cc <- as.data.frame(rbind( knn_tfidf_cc_red_results_mean, rf_tfidf_cc_red_results_mean,xgb_tfidf_cc_red_results_mean))
performance_cc[["model"]] <- c('knn','rf','xgb')
performance_cc <- performance_cc[names(performance_cc) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric_br <- gather(performance_br,metrics,value,-model)
perf_metric_lp <- gather(performance_lp,metrics,value,-model)
perf_metric_cc <- gather(performance_cc,metrics,value,-model)

library(gridExtra)


g_br <- ggplot(perf_metric_br)+geom_tile(aes(x=model,y=metrics,fill=value),color="white")+geom_text(aes(x=model,y=metrics,label=round(value,3)),color="black")+scale_fill_distiller(palette = "Spectral")+ggtitle("BR")

g_lp <- ggplot(perf_metric_lp)+geom_tile(aes(x=model,y=metrics,fill=value),color="white")+geom_text(aes(x=model,y=metrics,label=round(value,3)),color="black")+scale_fill_distiller(palette = "Spectral")+ggtitle("LP")

g_cc <- ggplot(perf_metric_cc)+geom_tile(aes(x=model,y=metrics,fill=value),color="white")+geom_text(aes(x=model,y=metrics,label=round(value,3)),color="black")+scale_fill_distiller(palette = "Spectral")+ggtitle("CC")

grid.arrange(g_br, g_lp, g_cc, nrow = 2, ncol=2)

```

We can see from the above Fig LP and Random Forest stands as the clear winner. Therefore, let's compare the LP and Random-Forest classifier transoform methods with the non-reduced label models, ans try to ansere rhe research Q-



```{r mld_red_comp_plot, message=TRUE, include=TRUE,eval=TRUE }
rm(performance)

performance <- as.data.frame(rbind( rf_tfidf_lp_results_mean, rf_tfidf_lp_red_results_mean))
performance[["model"]] <- c("Original",'Reduced-label')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

ggplot(perf_metric)+geom_tile(aes(x=model,y=metrics,fill=value),color="black")+geom_text(aes(x=model,y=metrics,label=round(value,3)),color="black")+scale_fill_distiller(palette = "Spectral")



```
It seems the original models (without reduced labels) performed better with respect to all metrics. Therefore it is not always true that we might get better performance by reducing labels as mentioned in [mldr].