---
title: "Multilabel Classification of Legal Text (EurLex)"
output: html_notebook
link-citations: yes
csl: ../markdown/biomed-central.csl
bibliography: ../markdown/bibliography.bib
---
##Overview and motivation

A single text document often has multiple semantic aspects. A single news article related to politics may have aspects related to trade, technology and defense. Therefore, often a document needs to be tagged to multiple labels/categories, instead of a single category. Most of the classification algorithms deal with datasets which have a set of input features and only *one* output class. However, in reality the problem might be different from a typical binary or multiclass classification. An introduction of enormous amount of documents belonging to multiple categories in the legal domain, makes it an attractive area for employing automated solutions.

In this project we explore a public multi labelled legal text dataset that has been manually annotated over a decade. It contains laws related to the European Union, including treaties, legislation, case-law and legislative proposals, in 22 different languages. This is popularly known as the **EUR-Lex** dataset containing about twenty thousand documents, around seven thousand labels and in several European languages. A skewed distribution of multiple labels per document, along with existence of the same data in multiple languages, makes this dataset an interesting proposition. Few publications have used an older version of the dataset which had around four thousand labels. The ones that have used this have reported relatively poor values in the range of 50% [@LozaMencía2010] (which may be fair, given the high number of labels). No publications for the new dataset, which is having around 7000 labels, motivates us to explore the problem of multilabel classification on this dataset. 

> **Multilable v/s Multiclass classification**  
In multi-label classification, each instance in the training set is associated with a set of labels, instead of a single lable, and the task is to predict the *label-sets* of unseen instances, instead of a single label. There is a difference between *multi-class classification* and *multi-label classification*. In multi-class problem the classes or labels are mutually-exclusive, i.e. it makes the assumption that each instance can be assigned to only one label. E.g - an animal can be either a dog or a cat but not both. But in multi-label problem multiple labels may be assigned to an instance. E.g - a movie can belong to a comedy genre as well a detective genre.


##Project objectives

In this project we first perform a statistical exploratory analysis of the dataset. Secondly, we experiment the performance of various state-of-the-art classifiers on this dataset. For a better understanding of this classification task, we study the evaluation measures for the multilabel scenario.
In the process, we try to answer the following research questions:  
  
* How well the classifiers perform over Eur-Lex dataset for two languages (English and Deutsch).  
* How the classifiers' performance changes with different features- one with term frequency–inverse document frequency(tf-idf), another with term incidence.
* Which flavour of multilabel classifiers perform best among all.
* How the classifiers' performance changes when the number of labels is reduced.

## Design overview (algorithms and methods we have used)</span>

* Pre-processing:
     + Exclude stop words, perform lemmatization.
     + Extract features - term frequency–inverse document frequency(tf-idf) and term incidence.
     + Generate the MLD [@Gibaja:2015:TML:2737799.2716262] data format, which is needed for multi label data exploration and classification using _mldr_[@charte2015working] and _utiml_[@rivolliutiml] packages.
     
* Statistical exploration:
     + Basic exploration  - distribution  of attributes/labels
     + Multi-label specific exploration- labelset distribution, relationship among labels, and relationship between attributes and labels/labelsets

* Classification:  
     + Apply the classifiers (Nearest Neighbour, Random Forest, XGBoost) over the preprocessed dataset (tf-idf and term incidence) for German and English text, and for three flavours of multilabel classification methods:  
           - Binary Relevance (BR) [@godbole2004discriminative]
           - Label Powerset (LP) [@boutell2004learning]
           - Classifier Chain (CC) [@read2011classifier]
     
     + Apply the classifiers (Nearest Neighbour, Random Forest, XGBoost) over the preprocessed dataset (tf-idf and term incidence) for English text *for reduced labels*, and for two flavours of multilabel classification methods:  
           - Binary Relevance (BR)
           - Label Powerset (LP)

* The following evaluation measures cab be used to assess the multilabel predictive performance:
     + Accuracy
     + Hamming Loss
     + Micro Precision and Recall
     + Macro Precision and Recall
     
* Compare the accuracies of the state-of-the-art classifiers on this dataset, for:
  + Two languages (English and German)
  + Two kinds of features (tf-idf and incidence)
  + Different flavours of multilable classification
  + Reduced labels


## Data

### Name and source

European Union law documents (EUR-Lex).
The [data](https://ec.europa.eu/jrc/en/language-technologies/jrc-eurovoc-indexer#Download%20JEX) is located inside the software distributed by European Union.

### Data format

- The Eurlex dataset for every language comprises two files.  
- The documents(laws/treaties) and the document categories/labels is available in a cf file (acquis.cf).
- The content and the labels for each document has been stored in the file in the following way:  

    + Every **odd** line consists of *label-ids* and the *document-id* of a document. The labels and document-id is separted by a *#*.  
    + Every **even** line consists of the actual text.  

An example has been shown below in the diagram. On the 1st line there are two lable-ids - *3032, 525* and the document-id is *31958d1006(01)*, and the actual text is on 2nd line.

<center> ![[Fig1. Data format]](../markdown/Figs/acquis.png) </center>

&nbsp;
&nbsp;

The mapping between label-id and label-name has been provided in a XML. A small snippet of the xml has been provided below.

```xml
<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE DESCRIPTEUR SYSTEM "descripteur.dtd">
<DESCRIPTEUR LNG="EN" VERSION="4_3">
  <RECORD>
    <DESCRIPTEUR_ID>4444</DESCRIPTEUR_ID>
    <LIBELLE>abandoned land</LIBELLE>
  </RECORD>
</DESCRIPTEUR>
```

The tag *DESCRIPTEUR_ID* contains the label-id and *LIBELLE* contains the label name.
&nbsp;
&nbsp;

### Preprocessing

For our task two kinds of  preprocessing is needed:

- **Text preprocessing**     
Text data contains many characters which do not convey much information, like punctuations, white spaces, stop words, etc. In English, certain words like “is”, “the” is present in every document and does not help to discriminate two documents. But again, depending on the language and the task at hand, we need to deal with such characters differently.

- **Preprocessing to generate the data expected by mldr**  
To study the MLD datasets traits (label distribution, relationship among labels and label imbalance) and perform classification over MLD datasets we need to transform the available dataset into mldr format - a sparse ARFF file containing features and labels , and a XML file containing the label information.

We first start with text preprocessing and some exploratory analysis over English text.

```{r init, message=FALSE, warning=FALSE, include=TRUE}
## Required packages
source("../scripts/requirements.R")

## Functions needed
source("../scripts/preprocessing_utility.R")

## Project settings
source("../configuration/en.R")

```


The following code will load and preprocess the text.

```{r load_text, include=TRUE, eval=TRUE}
connection <- fileName  %>% file(open = "r")
raw_text_char <- connection %>% readLines(encoding = "UTF-8")
close.connection(connection)

sample_no <- ifelse((exists("doc_number") && doc_number>1), doc_number, length(raw_text_char))
text_content_list <- raw_text_char[seq (2, sample_no, 2)] #every even line contains text
text_content_list[[1]]
```

```{r preprocess_text, include=TRUE, eval=TRUE}
text_content_list <- get_clean_content(text_content_list) #get preprocessed text

text_content_list[[1]]
```

We need to preprocess labels, as label names are in different file. The dataset- *acquis.cf* file just contains label-ids. Also the label names contains certain characters like space which is inconvenient to generate MLD datasets. The following code generates clean label name for all documents.

```{r get_clean_label_lst, include=TRUE, eval=TRUE}
library(XML)

class_labels_list <-
  raw_text_char[seq (1, sample_no, 2)] %>%
  strsplit("#") %>%
  sapply("[[", 1) %>%
  trimws() %>%
  get_label_name_list()

raw_text_char[[1]] # first row containing label-ids for 1st document

class_labels_list[[1]] # corresponding label-names for 1st document
```

After generating a cleaner version of text and labels we can take a deep dive into the first part of our data exploration.

We generate a dataframe such that each row contains one document's text and one label.

```{r exploration_df, include=TRUE, eval=TRUE}
text <- character()
label <- character()
for (index in 1:length(text_content_list)) {
  temp_labelset <- unlist(class_labels_list[[index]])
  for (label_index in 1:length(temp_labelset))
  {
    text <- append(text, text_content_list[[index]])
  label <- append(label, temp_labelset[[label_index]])
  }
}

text_df <- as.data.frame(cbind(text, label), stringsAsFactors = FALSE)

text_df[1:4,]
```

### Exploratory data analysis

We start exploration with wordcloud, which is a simple yet informative way to understand textual data and perform analysis.

```{r wordcloud_all, include=TRUE, eval=TRUE}
library(tidytext)

#generate tokens and count of each word from text 
tokens <- text_df %>%
  unnest_tokens(word, text) %>%
  dplyr::count( word, sort = TRUE) %>%
  ungroup()

library(wordcloud)

wordcloud(words = tokens$word, freq = tokens$n, min.freq = 1, max.words=50, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
```

The word cloud has been generated after adding some law and the dataset related stopwords like -*treaty, journal, europe*. We see that the word *state* among others, stands out and it seems the laws/treaties are related to a state more than a country. After getting some insight from the wordcloud we examine it further. It will be interesting to see which terms are common for different category of legal text. To get that, we need to count words for each category.

```{r tf, include=TRUE, eval=TRUE}
tokens_by_label <- text_df %>%
  unnest_tokens(word, text) %>% #generate tokens
  dplyr::count(label, word, sort = TRUE) %>% #counts words for each category
  ungroup()
  
  total_words <- tokens_by_label %>%
    group_by(label) %>%
    summarize(total = sum(n))
  
  tokens_by_label <- left_join(tokens_by_label, total_words)
  
  tokens_by_label 
```

There is one row in the data frame *tokens_by_label* for each word-label combination. *n* is the number of times that word is used in that legal text category/label and *total* is the total words in that category. Let us have a look at the distribution of (n/total) for each document category- the number of times a word appears in a label divided by the total number of terms in that category. Our dataset has around 6797 labels , and it will be difficult to analyse over all categories! Therfore, we will consider some of the labels for our analysis.

```{r tf_graph, include=TRUE, eval=TRUE, warning=FALSE}
library(ggplot2)

selected_labels <- unique(tokens_by_label$label)[1:8]

tokens_by_label %>% filter(
  label %in% selected_labels) %>%
  ggplot(aes(n / total, fill = label)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap( ~ label, ncol = 2, scales = "free_y")
```

Some of the plots portray long tails on the right, and we can use the term frequency dataframe to plot term frequency and examine Zipf's law.

>> Zipf's law states that the frequency that a word appears is inversely proportional to its rank.

```{r zipf, include=TRUE, eval=TRUE}


freq_by_rank <- tokens_by_label %>%
  group_by(label) %>%
  mutate(rank = row_number(),
  `term frequency` = n / total)

freq_by_rank %>%
  ggplot(aes(rank, `term frequency`, color = label)) +
  geom_line(size = 1.1,
  alpha = 0.8,
  show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()

```

The above plot is in log-log coordinates. We can see that text in all the categories in the corpus are almost similar to each other, and that the relationship between rank and frequency does have negative slope, implying that they follow Zipf's Law.

It would be interesting to find out top 10 words for each category, which will give us an idea what each category of law document deals with. To find the most importants words it would make sense to use tf-idf rather than term frequency, as tf-idf finds the important words for the content of each document category by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents.  

```{r topk_mono, include=TRUE, eval=TRUE}

tokens_by_label <- bind_tf_idf(tokens_by_label, word, label, n) #enerates tf-idf and binds to dataframe
tokens_by_label
```

```{r topk_plot, include=TRUE, eval=TRUE}

tokens_by_label %>%
  filter(
  label %in% selected_labels[1:4] ) %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(word, levels = rev(unique(word)))) %>%
  group_by(label) %>%
  top_n(10) %>%
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = label)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = NULL) +
  facet_wrap( ~ label, ncol = 2, scales = "free") +
  coord_flip()
```

We can see common terms appearing for labels *seed_* ,*fodder_plant_* and *marketing_standard_*. This implies many documents for marketing standard deals with seed and fodder plant. *approximation_of_laws* also has the term plant, which means it might also have documents related to plants.  
Sometimes bigrams are more meaningful than single words, and it would be interesting to see whether we make similar observation for bigrams.

We generate the tokens in similar way, except we use we used *token = "ngrams"* and *n=2* in the method *unnest_tokens.*

```{r topk_b_plot, include=TRUE, eval=TRUE}

bigram_tokens <- text_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  dplyr::count(label, bigram, sort = TRUE) %>%
  ungroup()
  
  total_words <- bigram_tokens %>%
  group_by(label) %>%
  summarize(total = sum(n))
  
  bigram_tokens <- left_join(bigram_tokens, total_words)
  bigram_tokens <- bind_tf_idf(bigram_tokens, bigram, label, n)
  
  bigram_tokens %>%
  filter(
  label %in% selected_labels[1:4]) %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
  group_by(label) %>%
  top_n(10) %>%
  ungroup %>%
  ggplot(aes(bigram, tf_idf, fill = label)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = NULL) +
  facet_wrap( ~ label, ncol = 2, scales = "free") +
  coord_flip()
```

We can see similar observation as seen in top-10 words.

Another way to view word connections is to treat them as a network, similar to a social network. Word networks show term association and cohesion. In a network graph, the circles are called nodes and represent individual terms, while the lines connecting the circles are called edges and represent the connections between the terms. We can see some interesting word association in the Fig below.

```{r worda, include=TRUE, eval=TRUE}
library(tidyr)
library(igraph)
library(ggraph)

(
  bigram_graph <- bigram_tokens %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  dplyr::count(word1, word2, sort = TRUE) %>%
  unite("bigram", c(word1, word2), sep = " ") %>%
  top_n(100) %>%
  graph_from_data_frame()
  )
  
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1, check_overlap = TRUE) +
  theme_void() 
```


### Preprocessing for mldr

To perform the exploratory analysis over the MLD dataset traits we need to generate the dataset in the format which can read by **mldr** package. First we generate the corpus and then document term matrix each for tf-idf and incidence datasets.

```{r dtms, include=TRUE, eval=TRUE}
#corpus of text
text_corpus <- VCorpus(VectorSource(text_content_list))

#corpus of labels just like text corpus
label_corpus <- VCorpus(VectorSource(class_labels_list))
#dtm of labels
dtm_labels <- DocumentTermMatrix(label_corpus, control=list(weight=weightTfIdf))

# dtm matrix for tf-idf. considering words having atleast 3 letters
dtm_tfidf <- text_corpus %>%
  DocumentTermMatrix(control = list(
  wordLengths = c(3, Inf),
  weighting = function(x)
  weightTfIdf(x, normalize = FALSE) ,
  stopwords = TRUE
  ))  %>%
  removeSparseTerms(0.99) # remove sparse terms, so that sparsity is maximum 99%
  
# column bind text and labels to generate tfidf ARFF file
dtm_tfidf <- cbind(dtm_tfidf, dtm_labels)

# gets unique words- needed for incidence matrix
  uniqueWords <- function(text) {
    return(paste(unique(strsplit(text, " ")[[1]]), collapse = ' '))
  }

# dtm matrix for incidence. considering words having atleast 3 letters
dtm_incidence <-  text_corpus %>%
    tm_map(content_transformer(uniqueWords)) %>%
    DocumentTermMatrix(control = list(
    wordLengths = c(3, Inf),
    weight = weightBin ,
    stopwords = TRUE
    )) %>%
    removeSparseTerms(0.99)

# column bind text and labels to generate incidence ARFF file
dtm_incidence <- cbind(dtm_incidence,dtm_labels)
```

After the document term matrices have been generated we need to generate the ARFFs and the xmls

```{r arff_gen, include=TRUE, eval=TRUE}

#generate ARFF for mldr
generate_ARFF(dtm_incidence, paste(incFileName,".arff",sep="")) #generate arff for incidence
generate_ARFF(dtm_tfidf, paste(tfidfFileName,".arff",sep="")) #generate arff for tfidf

#generate XML for mldr
label_names <-
xmlParse(labelFile) %>% xpathApply("//LIBELLE", xmlValue) %>% get_clean_label()
xml_root = newXMLNode("labels")

for (i in 1:length(label_names)) {
  newXMLNode("label", attrs = c(name = label_names[i]), parent = xml_root)
}

saveXML(xml_root,file=paste(incFileName,".xml", sep="")) #xml for incidence
saveXML(xml_root,file=paste(tfidfFileName,".xml", sep="")) #xml for tfidf
```

### Exploratory multilabel dataset analysis
After mldr compliant datasets are generated, they are loaded. Since our objective in performing exploratory analysis over the multilabel dataset is to inspect certain multi-label dataset traits (label distribution, relationship among labels and label imbalance), it is fine if we perform the analysis over one of the datasets (tf-idf or incidence). We are considering tf-idf dataset for our analysis.

```{r load_mldr, include=TRUE, cached=TRUE}
eurlex <- mldr(paste(tfidfFileName))

summary(eurlex)
```
As we know the value of scumble lies in the range [0,1], and a low score would denote an MLD with not much concurrence among imbalanced labels and the resampling algorithms would work better. The scumble value of our dataset is pretty high, implying there is a good amount of concurrence among imbalanced labels, making it a difficult classification problem. The mean IRLbl value (meanIR) is also quite high indicating a good amount of imbalance.  

#### labels' information
Labels' information in the MLD, including the number of times they appear, their IRLbl and SCUMBLE measures, can be retrieved by using the *labels* member of the *mldr* class.

```{r mldr_label, include=TRUE, cached=TRUE}
#inspect first 20 labels
head(eurlex$labels, 20)

```
#### Concurrence Plot (CH)
The concurrence plot explores interactions among labels. 
We can choose the labels from the mldr attribute *eurlex\$labels*. The plots of mldr use *eurlex\$attributes*, and they contain all features and labels. The *eurlex\$attributes* first start with features and then followed by labels. We choose some labels for the plot, as plotting all of them would produce a very confusing result.

```{r mldr_attrib, include=TRUE, cached=TRUE }
head(eurlex$attributes)

tail(eurlex$attributes)

#selecting first 20 labels/categories
plot_labels <- eurlex$labels$index[1:40]
names(eurlex$attributes)[plot_labels]
```

The concurrence plot explores interactions among labels. This plot has a circular shape, with the circumference partitioned into many disjoint arcs representing labels. Each arc has length proportional to the number of instances where the label is present. These arcs are in turn divided into bands that join two of them, showing the relation between the corresponding labels. The width of each band is proportional to the number of instances in which both labels appear simultaneously. Since drawing interactions among all the labels can produce a confusing result, we have plotted for some labels.
We can plot the concurrence plot using the plot() using type "LC"

```{r plot_lc, include=TRUE, eval=TRUE }
plot(eurlex, type="LC", labelIndices=plot_labels, title="Concurrence plot")

```

We can see a thick blue band joining cattle_ and ani_ase(animal_disease), which means there are instances common to labels animal_disease_ and cattle_. There are no bands connecting common_agricultural_policy_(com_icy) and animal_disease (ani_ase), which means there are no common instances between them.

#### Label histogram Plot (LH)
The label histogram relates labels and instances in a way that shows how well-represented labels are in general. The X axis is assigned to the number of instances and the Y axis to the amount of labels. In our plot the data is concentrated on the left side of the plot, which means that a great number of labels are appearing in very few instances.

```{r plot_lh, include=TRUE, eval=TRUE }
plot(eurlex, type='LH',col = brewer.pal(11, 'Spectral'), title='Eurlex')
```

We can see most of the instances have very less labels considering the total number of labels is around 7000.

#### Labelset histogram (LSH)
The labelset histogram is similar to LH plot but, instead of representing the number of instances in which each label appears, it shows the amount of labelsets. We can make similar observation as the LH plot.

```{r plot_lsh, include=TRUE, eval=TRUE }
plot(eurlex, type='LSH',col = brewer.pal(11, 'Spectral'), title='Eurlex', ylim=c(0,100))
```

#### Cardinality histogram (CH)
The cardinality histogram represents the amount of labels instances have in general. Since data has accumulated on the left side of the plot it means that very less instances have a notable amount of labels.

```{r plot_ch, include=TRUE, eval=TRUE }
plot(eurlex, type='CH',col = brewer.pal(11, 'Spectral'), title='Eurlex')
```

We can see not many instances are assigned to a category/label, considering the huge number of documents we have (around 24,000).

## Multilabel Classification

There are two possibilities to deal with multi-label classification:  

- Algorithm adaptation: Modify existing algorithms taking into account the multilabel nature of the samples, for instance hosting more than one class in the leaves of a tree instead of only one.

- Problem transformation: Transforming the original data to make it suitable to existing traditional classification algorithms and combining the obtained predictions to build the labelsets given as output result. There are several transformation methods in literature. Three have been defined and used for our case study.  

    + Binary Relevance (BR): It is an adaptation of OVA (one-vs-all) to the multilabel scenario and transforms the original multilabel dataset into several binary datasets. Each classifier predicts either the membership or the non-membership of one class. A union of all predicted classes is taken as the multi-label output. The approach is faster and easy to implement, but *ignores the possible correlations between class labels*.  

    + Label Powerset (LP): This method transforms the multilabel dataset into a multiclass dataset by using the labelset of each instance as class identifier. This approach takes possible correlations between class labels into account unlike the BR. The downside of the method is it has a high computational complexity and when the number of classes increases the number of distinct label combinations can grow exponentially.

    +  Classifier Chains (CC): This method comprises a chain of binary classifiers \(C_0, C_1, . . . , C_m \) is constructed, where a classifier \(C_i\) uses the predictions of all the classifier \(C_j\) , where j < i. The method can takes into account label correlations. The total number of classifiers needed for this approach is equal to the number of classes, but the training of the classifiers is more involved. 

We have used BR, LP and CC for our problem, and we will try to answer the research question: *Which flavour of multilabel classification method performs the best*.

To perform classification we generate multiple batches of datasets (arff and xml), as mldr is very slow in loading large arff files and performing classification.

```{r generate_dataset_clsf, message=TRUE, include=TRUE,eval=TRUE }

## Generate dataset for classification
source("../scripts/generate_dataset.R")
```

### Classification for tf-idf dataset using Binary Relevance Transform (BR)

The method *classify* performs classification over the specified dataset, for one of the multilabel flavour (Binary relevance -*br*, Labelpowerset- *lp* or Classifier Chain-*cc*), and for one of the classifiers (Nearest Neighbour-*KNN*, Random Forest-*RF*, XGBoost-*XGB*). The last input to method classify is a flag which determines whether to consider reduced labels or not. The method returns the average of all the metrics' value over the batches of datasets.
```{r br_tf, message=TRUE, include=TRUE,eval=TRUE }
source("../scripts/classification_utility.R")
set.seed(123) #insures to reproduce results.

knn_tfidf_br_results_mean <- classify(tfidfFileName,"br","KNN", FALSE)
rf_tfidf_br_results_mean <- classify(tfidfFileName,"br","RF", FALSE)
xgb_tfidf_br_results_mean <- classify(tfidfFileName,"br","XGB", FALSE)
```


```{r br_tfidf_plot, message=TRUE, include=TRUE,eval=TRUE }
if(exists("performance")) rm(performance)

performance=as.data.frame(rbind( knn_tfidf_br_results_mean, rf_tfidf_br_results_mean, xgb_tfidf_br_results_mean))
performance[["model"]] <- c("knn",'rf','xgb')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0)) 
```

We can observe Random Forest clearly performs best among all, with respect to all metrics. KNN performs slightly better than XGB with respect to most of the metrics.

### Classification for tf-idf dataset using LabelPowerset Transform (LP)

```{r lp_tf, message=TRUE, include=TRUE,eval=TRUE }
knn_tfidf_lp_results_mean <- classify(tfidfFileName,"lp","KNN", FALSE)
rf_tfidf_lp_results_mean <- classify(tfidfFileName,"lp","RF", FALSE)
xgb_tfidf_lp_results_mean <- classify(tfidfFileName,"lp","XGB", FALSE)
```
We similarly plot the metrics for LP.

```{r lp_tfidf_plot, message=TRUE, include=TRUE,eval=TRUE }
rm(performance)

performance=as.data.frame(rbind( knn_tfidf_lp_results_mean, rf_tfidf_lp_results_mean, xgb_tfidf_lp_results_mean))
performance[["model"]] <- c("knn",'rf','xgb')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0))

```

Here also we observe Random Forest performs best among all and XGB worst, with respect to all metrics.

### Classification for tf-idf dataset using Classifier Chain Transform (CC)

```{r cc_tfidf, message=TRUE, include=TRUE,eval=TRUE }
knn_tfidf_cc_results_mean <- classify(tfidfFileName,"cc","KNN", FALSE)
rf_tfidf_cc_results_mean <- classify(tfidfFileName,"cc","RF", FALSE)
xgb_tfidf_cc_results_mean <- classify(tfidfFileName,"cc","XGB", FALSE)

```

We similarly plot for CC.

```{r cc_tfidf_plot, message=TRUE, include=TRUE,eval=TRUE }
rm(performance)

performance <- as.data.frame(rbind( knn_tfidf_cc_results_mean, rf_tfidf_cc_results_mean, xgb_tfidf_cc_results_mean))
performance[["model"]] <- c("knn",'rf','xgb')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0))

```
Here also we observe Random Forest perfoms best among all, and KNN has performed similarly with respect to some metrics.

### Comparison among MLD algorithms over tf-idf dataset
Now let's see which flavour of MLD algorithms performs best. Considering only random forest classifiers for BR, LP and CC, as it is the clear winner in all the cases.

```{r mld_tfidf_plot, message=TRUE, include=TRUE,eval=TRUE }
rm(performance)
performance <- as.data.frame(rbind( rf_tfidf_br_results_mean, rf_tfidf_lp_results_mean, rf_tfidf_cc_results_mean))
performance[["model"]] <- c("br",'lp','cc')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

library(gridExtra)

g1 <- ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0))

g2 <- ggplot(perf_metric)+geom_tile(aes(x=model,y=metrics,fill=value),color="black")+geom_text(aes(x=model,y=metrics,label=round(value,3)),color="black")+scale_fill_distiller(palette = "Spectral")

grid.arrange(g1, g2, nrow = 1)

```

We observe LP is the clear winner with respect to all metrics. Since LP is the winner it implies that it does not treat the labels independently. BR works well for those scenarios where labels are treated independently and Chained models (CC) works better when there is a clear hierarchical or causative relationship among the labels.

### Classification for incidence dataset

We similar perform classification over the incidence dataset using BR, LP and CC.

```{r inc_all , message=TRUE, include=TRUE,eval=TRUE }
knn_inc_br_results_mean <- classify(incFileName,"br","KNN", FALSE)
rf_inc_br_results_mean <- classify(incFileName,"br","RF", FALSE)
xgb_inc_br_results_mean <- classify(incFileName,"br","XGB", FALSE)

knn_inc_lp_results_mean <- classify(incFileName,"lp","KNN", FALSE)
rf_inc_lp_results_mean <- classify(incFileName,"lp","RF", FALSE)
xgb_inc_lp_results_mean <- classify(incFileName,"lp","XGB", FALSE)

knn_inc_cc_results_mean <- classify(incFileName,"cc","KNN", FALSE)
rf_inc_cc_results_mean <- classify(incFileName,"cc","RF", FALSE)
xgb_inc_cc_results_mean <- classify(incFileName,"cc","XGB", FALSE)
```

```{r br_inc_plot, message=TRUE, include=TRUE,eval=TRUE }
rm(performance)
performance <- as.data.frame(rbind( knn_inc_br_results_mean, rf_inc_br_results_mean, xgb_inc_br_results_mean))
performance[["model"]] <- c("knn",'rf','xgb')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0))

```

```{r br_lp_inc_plot, message=TRUE, include=TRUE,eval=TRUE }
rm(performance)
performance <- as.data.frame(rbind( knn_inc_lp_results_mean, rf_inc_lp_results_mean, xgb_inc_lp_results_mean))
performance[["model"]] <- c("knn",'rf','xgb')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0))

```

 
```{r cc_inc_plot, message=TRUE, include=TRUE,eval=TRUE }
rm(performance)
performance=as.data.frame(rbind( knn_inc_cc_results_mean, rf_inc_cc_results_mean, xgb_inc_cc_results_mean))
performance[["model"]] <- c("knn",'rf','xgb')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0))

```

In all the plots for incidence also we observe Random Forest is the winner. Let's see which flavour of MLD classification performs best.

### Comparison of MLD Classification Algorithm over incidence dataset

Here also we only consider Random Forest, as it has performed for all - LP, BR, CC.

```{r mld_inc_plot, message=TRUE, include=TRUE,eval=TRUE }
rm(performance)
performance <- as.data.frame(rbind( rf_inc_br_results_mean, rf_inc_lp_results_mean, rf_inc_cc_results_mean))
performance[["model"]] <- c("br",'lp','cc')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

library(gridExtra)

g1 <- ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0))

g2 <- ggplot(perf_metric)+geom_tile(aes(x=model,y=metrics,fill=value),color="black")+geom_text(aes(x=model,y=metrics,label=round(value,3)),color="black")+scale_fill_distiller(palette = "Spectral")

grid.arrange(g1, g2, nrow = 1, ncol=2)

```

Here also LP performs the best, except for *Micro-Precision*.

Now let us see which features proved to be better. We consider only Random Forest here also for the same before mentioned reason.
```{r mld_inc_tfidf_plot, message=TRUE, include=TRUE,eval=TRUE }
rm(performance)
performance <- as.data.frame(rbind( rf_tfidf_lp_results_mean, rf_inc_lp_results_mean))
performance[["model"]] <- c('tf-idf','incidence')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

library(gridExtra)

g1 <- ggplot(perf_metric)+geom_point(aes(x=model,y=value,color=model),size=5,alpha=0.7)+facet_grid(metrics~.,scales='free_x', space='free_x')+coord_flip()+theme_bw()+scale_color_manual(values=mycolors)+theme(strip.text.y = element_text(angle=0))

g2 <- ggplot(perf_metric)+geom_tile(aes(x=model,y=metrics,fill=value),color="black")+geom_text(aes(x=model,y=metrics,label=round(value,3)),color="black")+scale_fill_distiller(palette = "Spectral")

grid.arrange(g1, g2, nrow = 1, ncol=2)

```

Both of them performed similar and it is difficult to say which one is better than the other.


Classification for tf-idf and incidence dataset done similarly by using BR and LP, but using reduced labels. By reducing labels we mean the majority labels are disabled on instances with highly imbalanced labels. We will try to answer the research question whether classification accuracy improves with reduced labels. We are considering only tf-idf dataset as performance of the classifiers was comparable.

```{r cls_red_labels , message=TRUE, include=TRUE,eval=TRUE }
knn_tfidf_br_red_results_mean <- classify(tfidfFileName,"br","KNN", TRUE)
rf_tfidf_br_red_results_mean <- classify(tfidfFileName,"br","RF", TRUE)
xgb_tfidf_br_red_results_mean <- classify(tfidfFileName,"br","XGB", TRUE)

knn_tfidf_lp_red_results_mean <- classify(tfidfFileName,"lp","KNN", TRUE)
rf_tfidf_lp_red_results_mean <- classify(tfidfFileName,"lp","RF", TRUE)
xgb_tfidf_lp_red_results_mean <- classify(tfidfFileName,"lp","XGB", TRUE)

knn_tfidf_cc_red_results_mean <- classify(tfidfFileName,"cc","KNN", TRUE)
rf_tfidf_cc_red_results_mean <- classify(tfidfFileName,"cc","RF", TRUE)
xgb_tfidf_cc_red_results_mean <- classify(tfidfFileName,"cc","XGB", TRUE)
```

```{r mld_red_label_plot, message=TRUE, include=TRUE,eval=TRUE }
if(exists("performance_br"))rm(performance_br)

performance_br <- as.data.frame(rbind( knn_tfidf_br_red_results_mean, rf_tfidf_br_red_results_mean,xgb_tfidf_br_red_results_mean))
performance_br[["model"]] <- c('knn','rf','xgb')
performance_br <- performance_br[names(performance_br) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

if(exists("performance_lp"))rm(performance_lp)

performance_lp <- as.data.frame(rbind( knn_tfidf_lp_red_results_mean, rf_tfidf_lp_red_results_mean,xgb_tfidf_lp_red_results_mean))
performance_lp[["model"]] <- c('knn','rf','xgb')
performance_lp <- performance_lp[names(performance_lp) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

if(exists("performance_cc"))rm(performance_cc)

performance_cc <- as.data.frame(rbind( knn_tfidf_cc_red_results_mean, rf_tfidf_cc_red_results_mean,xgb_tfidf_cc_red_results_mean))
performance_cc[["model"]] <- c('knn','rf','xgb')
performance_cc <- performance_cc[names(performance_cc) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric_br <- gather(performance_br,metrics,value,-model)
perf_metric_lp <- gather(performance_lp,metrics,value,-model)
perf_metric_cc <- gather(performance_cc,metrics,value,-model)

library(gridExtra)


g_br <- ggplot(perf_metric_br)+geom_tile(aes(x=model,y=metrics,fill=value),color="white")+geom_text(aes(x=model,y=metrics,label=round(value,3)),color="black")+scale_fill_distiller(palette = "Spectral")+ggtitle("BR")

g_lp <- ggplot(perf_metric_lp)+geom_tile(aes(x=model,y=metrics,fill=value),color="white")+geom_text(aes(x=model,y=metrics,label=round(value,3)),color="black")+scale_fill_distiller(palette = "Spectral")+ggtitle("LP")

g_cc <- ggplot(perf_metric_cc)+geom_tile(aes(x=model,y=metrics,fill=value),color="white")+geom_text(aes(x=model,y=metrics,label=round(value,3)),color="black")+scale_fill_distiller(palette = "Spectral")+ggtitle("CC")

grid.arrange(g_br, g_lp, g_cc, nrow = 2, ncol=2)

```

We can see from the above Fig LP and Random Forest stands as the clear winner. Therefore, let's compare the LP and Random-Forest classifier transoform methods with the non-reduced label models, ans try to ansere rhe research Q-



```{r mld_red_comp_plot, message=TRUE, include=TRUE,eval=TRUE }
rm(performance)
performance <- as.data.frame(rbind( rf_tfidf_lp_results_mean, rf_tfidf_lp_red_results_mean))
performance[["model"]] <- c("Original",'Reduced-label')
performance <- performance[names(performance) %in% c('accuracy', 'hamming-loss', 'subset-accuracy','macro-F1', 'macro-precision', 'macro-recall','micro-F1', 'micro-precision', 'micro-recall','model')]

mycolors=c("#db0229","#026bdb","#48039e","#0d7502","#c97c02","#c40c09","#ff003f","#0094ff", "#ae00ff" , "#94ff00", "#ffc700","#fc1814","#f00814","#fc1874")

perf_metric <- gather(performance,metrics,value,-model)

ggplot(perf_metric)+geom_tile(aes(x=model,y=metrics,fill=value),color="black")+geom_text(aes(x=model,y=metrics,label=round(value,3)),color="black")+scale_fill_distiller(palette = "Spectral")



```
It seems the original models (without reduced labels) performed better with respect to all metrics. Therefore it is not always true that we might get better performance by reducing labels as mentioned in [mldr].