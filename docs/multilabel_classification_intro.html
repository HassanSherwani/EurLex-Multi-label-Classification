<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/united.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>




<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">EurLex classification</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="multilabel_classification_intro.html">Multi-label classification</a>
</li>
<li>
  <a href="exploratory_analysis.html">Exploratory Analysis</a>
</li>
<li>
  <a href="experimentalResults.html">Classification</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<p>     </p>
<div id="basics-of-multilabel-classification" class="section level1">
<h1>Basics of multilabel classification</h1>
<p>In multi-label classification, each instance in the training set is associated with a set of labels, instead of a single lable, and the task is to predict the label-sets of unseen instances, instead of a single label. Therefore, multi-label datasets (MLD) are different from binary/multi-class ones as they have multiple class per instance instead of one. Therefore each instance in MLD has a set of features(attributes) and a set of labels (labelsets). It is not rare in MLDs that there are more labels than features. Our dataset has around 7000 labels and lesser number of attributes. Our dataset like most MLDs is very imbalanced. The labels in a MLD can be correlated or not. Moreover, frequent labels and rare labels can appear together in the same instances.</p>
<div id="working-with-multilabel-datasets" class="section level2 tabset tabset-fade">
<h2>Working with multilabel datasets</h2>
<p>Binary and multiclass datasets can be handled in R by using dataframes and usually the last attribute is the output class, whether it contains only TRUE/FALSE values or a value belonging to a factor. As MLD have nultiple labels, some questions might arise in the mind of the readers:</p>
<p><span class="sub-sub-header">Can we use the existing data.frame for Multilabel datasets?</span> - Unfortunately we can’t!</p>
<p><span class="sub-sub-header">How can we handle multilabel datasets in R?</span> - <em>R data.frame</em> can be utilised for the Multilabel datasets (MLD), but an additional structure is required to understand which attributes are labels.</p>
<p><span class="sub-sub-header"> Then how do we handle it? </span> - To mitigate the issue the first R package introduced for the task is <a href="https://cran.r-project.org/web/packages/mldr/mldr.pdf">mldr</a>. It provides the user with the functions needed to perform exploratory analysis over MLDs, as well brings the data in the format suitable for use by the classification algorithms.<br />
The <em>mldr</em> needs two files:</p>
<ul>
<li>An <a href="https://www.cs.waikato.ac.nz/ml/weka/arff.html"><em>ARFF</em></a> file containing the attributes and labelsets information<br />
</li>
<li>A <em>xml</em> file (has to be same name as ARFF file) which contains the mapping between the label-id and label names.</li>
</ul>
<p>There are some apects typical to MLDs which we should be aware of, before we dive into the problem of multi-label classification. The three major aspects have been discussed below:</p>
<ul>
<li>Multilabel dataset traits</li>
<li>Multi-label classification</li>
<li>Evaluation metric for multilabel classification</li>
</ul>
<div id="multilabel-dataset-traits" class="section level3">
<h3>Multilabel dataset traits</h3>
<p>The figure below outlines the measures. Some of the important measures have been explained below.</p>
<center>
<img src="Figs/measures.png" alt="measures for MLD" />
</center>
<p><span class="sub-header">Basic measures :</span></p>
<p>The most basic information that can be obtained from an MLD is the number of instances, attributes and labels. Each instance has an associated labelset, whose length (number of active labels) can be in the range {0..|L|}.</p>
<p><span class="sub-header">Label related measures :</span></p>
<p><span class="sub-sub-header"> SCUMBLE (Score of ConcUrrence among iMBalanced LabEls):</span> The joint presence of labels with different frequencies in the same instance can pose a challenge for resampling algorithms. It is used to assess the concurrence level among frequent and infrequent labels.</p>
<p><span class="sub-sub-header"> Card (Cardinality):</span> The average number of active labels per instance.</p>
<p><span class="sub-sub-header"> Dens (Density):</span> Dividing * Card * by the number of labels results in a dimension-less measure, known as Dens.</p>
<p><span class="sub-sub-header"> UniqLabelsets:</span> The number of different labelsets and the amount of them being unique (appearing only once in sample). This gives us a glimpse on how sparsely the labels are distributed.</p>
<p><span class="sub-header">Imbalance related measures:</span></p>
<p><span class="sub-sub-header"> IRLb (Imbalance ratio per label):</span> The level of imbalance of a determinate label can be measured by the imbalance ratio, known as IRLb. it is calculated for the label <em>l</em> as the ratio between the majority label and the label <em>l</em>. This value will be 1 for the most frequent label and a greater value for the rest. The larger the IRLbl is, the higher would be the imbalance level for the considered label.</p>
</div>
<div id="multilabel-classification" class="section level3">
<h3>Multilabel classification</h3>
<p>There are two possibilities to deal with multi-label classification:<br />
<span class="sub-header"> Algorithm adaptation:</span><br />
Modify existing algorithms taking into account the multilabel nature of the samples, for instance hosting more than one class in the leaves of a tree instead of only one.</p>
<p><span class="sub-header"> Problem transformation:</span><br />
Transforming the original data to make it suitable to existing traditional classification algorithms and combining the obtained predictions to build the labelsets given as output result. There are several transformation methods in literature. Three have been defined and used for our case study.</p>
<ul>
<li><span class="sub-sub-header"> Binary Relevance (BR):</span>
<ul>
<li>It is an adaptation of OVA (one-vs-all) to the multilabel scenario and transforms the original multilabel dataset into several binary datasets.</li>
<li>Here an ensemble of binary classifiers is trained, one for each class. Each classifier predicts either the membership or the non-membership of one class. A union of all predicted classes is taken as the multi-label output.</li>
<li>The approach is popular because it is easy to implement, but <em>ignores the possible correlations between class labels</em>.</li>
</ul></li>
<li><span class="sub-sub-header"> Label Powerset (LP):</span>
<ul>
<li>This method transforms the multilabel dataset into a multiclass dataset by using the labelset of each instance as class identifier.</li>
<li>This approach takes possible correlations between class labels into account unlike the BR.</li>
<li>The downside of the method is it has a high computational complexity and when the number of classes increases the number of distinct label combinations can grow exponentially.</li>
</ul></li>
<li><span class="sub-sub-header"> Classifier Chains (CC):</span>
<ul>
<li>This method comprises a chain of binary classifiers <span class="math inline">\(C_0, C_1, . . . , C_m \)</span> is constructed, where a classifier <span class="math inline">\(C_i\)</span> uses the predictions of all the classifier <span class="math inline">\(C_j\)</span> , where j &lt; i.</li>
<li>The method can takes into account label correlations.</li>
<li>The total number of classifiers needed for this approach is equal to the number of classes, but the training of the classifiers is more involved.</li>
</ul></li>
</ul>
</div>
<div id="evaluation-metric" class="section level3">
<h3>Evaluation metric</h3>
<p>Evaluation measures for a multi-label classification problem needs discussion as it is different from multiclass/binary class problem. In single label classification the commonly used metrics are - accuracy, precision, recall, F1-measure, among others. In multi-label classification we cannot define misclassification as a hard correct or incorrect, but a prediction comprising subset of actual classes is deemed better than containg none of them.</p>
<p>Multilabel evaluation metrics are grouped into two main categories: example based and label based metrics.</p>
<ul>
<li><strong>Example based metrics</strong> are computed individually for each instance, then averaged to obtain the final value.</li>
<li><strong>Label based metrics</strong> are computed per label, instead of per instance.</li>
</ul>
<p>Some of the measures have been described below.</p>
<p><span class="sub-header"> Hamming Loss (Example based)</span><br />
Hamming Loss is is an example based measure. It is defined as the fraction of labels that are incorrectly predicted.</p>
<p><span class="math inline">\(HL = \frac{1}{N . L} \sum_{l=1}^L\sum_{i=1}^N Y_{i,l} \oplus X_{i,l}\)</span></p>
<p>where denotes exlusive-or, <span class="math inline">\(X_{i,l} (Y_{i,l})\)</span> stands for boolean that the i-th prediction contains the l-th label. For binary scenario (L=1) equals to (1 - accuracy).</p>
<p><span class="sub-header"> Micro-average and Macro-average (Label based)</span><br />
In order to measure the performance of a multi-class classifier we have to consider the average performance over all classes. There are two different ways of doing this called micro-averaging and macro-averaging. + A <strong>macro-average</strong> will compute the metric independently for each class and then take the average (hence treating all classes equally) + A <strong>micro-average</strong> will aggregate the contributions of all classes to compute the average metric.</p>
<p>In a multi-class classification setup, micro-average is <em>preferable</em> if is class imbalance.</p>
<p><span class="sub-sub-header"> Micro Average</span><br />
In micro, all TPs, TNs, FPs and FNs for each class are summed up and then the average is taken. The micro-average F1 is the harmonic mean of the below two equations.</p>
<p><span class="math inline">\(Microaverage Precision Prc^{micro}(D) = \frac{\sum_c TP_c}{\sum_c TP_c + \sum_c FP_c} \)</span></p>
<p><span class="math inline">\(Microaverage Recall Rcl^{micro}(D) = = \frac{\sum_c TP_c}{\sum_c TP_c + \sum_c FN_c} \)</span></p>
<p><span class="sub-sub-header"> Macro Average</span><br />
In macro average we take the average of precision and recall of the system on different sets. It is used when we want to know how the algorithm performs overall across different subset of data.</p>
<p><span class="math inline">\(Macrooaverage Precision Prc^{macro}(D) = \frac{\sum_c Prc(D,c)}{|C|} \)</span></p>
<p><span class="math inline">\(Microaverage Recall Rcl^{macro}(D) = \frac{\sum_c Rcl(D,c)}{|C|} \)</span></p>
<p><span class="sub-header"> Subset accuracy (Example based)</span><br />
It is also called subset accuracy, is the most strict evaluation metric. It indicates the percentage of samples that have all their labels classified correctly. The downside of this measure is that it is too strcit to be true, i.e - it ignores the partially correct matches. In a dataset having huge number of labels it is very challenging to get a good score for this measure.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
