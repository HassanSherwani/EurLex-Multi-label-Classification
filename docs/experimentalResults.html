<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/united.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #0000ff; } /* Keyword */
code > span.ch { color: #008080; } /* Char */
code > span.st { color: #008080; } /* String */
code > span.co { color: #008000; } /* Comment */
code > span.ot { color: #ff4000; } /* Other */
code > span.al { color: #ff0000; } /* Alert */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #008000; font-weight: bold; } /* Warning */
code > span.cn { } /* Constant */
code > span.sc { color: #008080; } /* SpecialChar */
code > span.vs { color: #008080; } /* VerbatimString */
code > span.ss { color: #008080; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { } /* Variable */
code > span.cf { color: #0000ff; } /* ControlFlow */
code > span.op { } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #ff4000; } /* Preprocessor */
code > span.do { color: #008000; } /* Documentation */
code > span.an { color: #008000; } /* Annotation */
code > span.cv { color: #008000; } /* CommentVar */
code > span.at { } /* Attribute */
code > span.in { color: #008000; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4,h5,h6",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = false;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">EurLex classification</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="multilabel_classification_intro.html">Multi-label classification</a>
</li>
<li>
  <a href="exploratory_analysis.html">Exploratory Analysis</a>
</li>
<li>
  <a href="experimentalResults.html">Classification</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<p>     </p>
<div id="classification" class="section level2">
<h2>Classification</h2>
<p>The EUR-Lex dataset contains 25K documents, which makes it impossible to train a classifier over the whole dataset. We have divided the dataset into 25 subsets, therefore each subset contains 1000 documents. We trained the classifiers over the subsets. The final evaluations results will be the average over the subsets.</p>
<div id="classification-models" class="section level4">
<h4><span class="sub-header">Classification Models</span></h4>
<p>We have used three multilabel transformation methods : Binary relevance (BR), Label powerset (LP), classifier chain (CC), to transform the dataset into a format, which can be used along existing classification algorithms - Random Forest (RF), k nearest neighbors (KNN), XGboosted trees (XGB). We have used the <em>knn</em> classifier from the package <a href="https://www.rdocumentation.org/packages/kknn/versions/1.3.1/topics/kknn">kknn</a>, <em>Random Forest</em> from <a href="https://www.rdocumentation.org/packages/randomForest">randomForest</a> and <em>XGBoost</em> from <a href="https://cran.r-project.org/web/packages/xgboost/xgboost.pdf">xgboost</a>. We have used the default parameters for the packages (kknn, randomforest, xgboost) for the classification task.</p>
<table class="kable_wrapper table table-striped table-hover table-condensed" style="width: auto !important; margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td>
<table>
<thead>
<tr>
<th style="text-align:left;">
Classifier Models
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
————————–
</td>
</tr>
<tr>
<td style="text-align:left;">
KNN
</td>
</tr>
<tr>
<td style="text-align:left;">
XGboost
</td>
</tr>
<tr>
<td style="text-align:left;">
Random Forest
</td>
</tr>
</tbody>
</table>
</td>
<td>
<table>
<thead>
<tr>
<th style="text-align:left;">
Transform Methods
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
————————–
</td>
</tr>
<tr>
<td style="text-align:left;">
Binary relevance
</td>
</tr>
<tr>
<td style="text-align:left;">
Classifier chain
</td>
</tr>
<tr>
<td style="text-align:left;">
Label powerset
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>We have trained nine classification models over 4 datasets - English tf-idf, English term incidence, German tf-idf, German term incidence and compared the performance of the classifiers, to answer the research questions:</p>
<ul>
<li>How well the classifiers perform over Eur-Lex dataset for two languages (English and Deutsch).<br />
</li>
<li>How the classifiers’ performance changes with different features- one with term frequencyâ€“inverse document frequency(tf-idf), another with term incidence.<br />
</li>
<li>Which flavour of multilabel transform algorithm perform best among all, the one which considers label correlation or the one which does not.<br />
</li>
<li>How the classifiers’ performance changes when the number of labels is reduced.</li>
</ul>
<p>The answers to these questions will help us understand whether we can apply machine learnin techniques to automatically categorize the legal text.</p>
<p>The following table shows the nine classifier models :</p>
<table class="table table-striped table-hover table-condensed table-responsive" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td style="text-align:left;">
KNN-Label Powerset
</td>
<td style="text-align:left;">
RF-Label Powerset
</td>
<td style="text-align:left;">
XGboost-Label Powerset
</td>
</tr>
</tbody>
</table>
<table class="table table-striped table-hover table-condensed table-responsive" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td style="text-align:left;">
KNN-Binary Relevance
</td>
<td style="text-align:left;">
RF-Binary Relevance
</td>
<td style="text-align:left;">
XGboost-Binary Relevance
</td>
</tr>
</tbody>
</table>
<table class="table table-striped table-hover table-condensed table-responsive" style="margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td style="text-align:left;">
KNN-Classifier Chain
</td>
<td style="text-align:left;">
RF-Classifier Chain
</td>
<td style="text-align:left;">
XGboost-Classifier Chain
</td>
</tr>
</tbody>
</table>
</div>
<div id="experimental-settings" class="section level4">
<h4><span class="sub-header">Experimental settings</span></h4>
<p>As mentioned earlier, the 25K documents dataset was split into 25 subsets.Each subset was split randomly into two disjoint subsets one for training and the other for testing, with the following proportions (65% used for training and 35% used for testing).</p>
<p>We reported the results of the different models under different settings. We wanted to explore the performance of the classifiers with two types of features, with two languages and with different number of labelsets. The following table demonstrates the experimental settings:</p>
<table class="kable_wrapper table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td>
<table>
<thead>
<tr>
<th style="text-align:left;">
Language
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
——–
</td>
</tr>
<tr>
<td style="text-align:left;">
English dataset
</td>
</tr>
<tr>
<td style="text-align:left;">
German dataset
</td>
</tr>
</tbody>
</table>
</td>
<td>
<table>
<thead>
<tr>
<th style="text-align:left;">
Features
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
——–
</td>
</tr>
<tr>
<td style="text-align:left;">
Tf-idf
</td>
</tr>
<tr>
<td style="text-align:left;">
Terms incidence
</td>
</tr>
</tbody>
</table>
</td>
<td>
<table>
<thead>
<tr>
<th style="text-align:left;">
Number of Labelsets
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
——–
</td>
</tr>
<tr>
<td style="text-align:left;">
14517
</td>
</tr>
<tr>
<td style="text-align:left;">
only balanced labelsets
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>The following code pertains to classification using BR transform.</p>
<p>To split the dataset into training and testing subsets we have used the function <a href="https://www.rdocumentation.org/packages/utiml/versions/0.1.4/topics/create_holdout_partition">create_holdout_partition()</a>. To perform classification using other transform methods we have to replace <a href="https://rdrr.io/cran/utiml/man/br.html">br()</a> for Binary Relevance with <a href="https://rdrr.io/cran/utiml/man/lp.html">lp()</a> for Label Powerset, and <a href="https://rdrr.io/cran/utiml/man/cc.html">cc()</a> for Classifier Chain. we had to destroy the model after storing the performance results to free memory using the function <em>rm()</em>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(mldr)
<span class="kw">library</span>(utiml)

train_ratio &lt;-<span class="st"> </span><span class="fl">0.65</span>
test_ratio &lt;-<span class="st"> </span><span class="fl">0.35</span>
iteration &lt;-<span class="st"> </span><span class="dv">25</span>

<span class="cf">for</span> (index <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>iteration) {
ds &lt;-<span class="st"> </span><span class="kw">mldr</span>(<span class="kw">paste</span>(generic_name, index, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)) <span class="op">%&gt;%</span>
<span class="kw">remove_unique_attributes</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="co">#remove attribute having same value for all labels</span>
<span class="kw">remove_unlabeled_instances</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="co">#remove instances having no labels</span>
<span class="kw">create_holdout_partition</span>(<span class="kw">c</span>(<span class="dt">train =</span> train_ratio, <span class="dt">test =</span> test_ratio)) <span class="co">#create holdout samples for train and test</span>

## KNN - K nearest neighbour
knn_model &lt;-<span class="st"> </span><span class="kw">br</span>(ds<span class="op">$</span>train, <span class="st">&quot;KNN&quot;</span>) <span class="co">#create model - BR +KNN</span>
knn_prediction &lt;-<span class="st"> </span><span class="kw">predict</span>(knn_model, ds<span class="op">$</span>test) <span class="co">#prediction for BR +KNN</span>
temp_knn &lt;-
<span class="kw">multilabel_evaluate</span>(ds<span class="op">$</span>test, knn_prediction, <span class="st">&quot;bipartition&quot;</span>) <span class="co">#get evaluation metric score  for BR +KNN</span>

<span class="co">#remove memory consuming variables</span>
<span class="kw">rm</span>(knn_model)
<span class="kw">rm</span>(knn_prediction)

## RF - Random Forest
rf_brmodel &lt;-<span class="st"> </span><span class="kw">br</span>(ds<span class="op">$</span>train, <span class="st">&quot;RF&quot;</span>) <span class="co">#create model - BR +RF</span>
rf_prediction &lt;-<span class="st"> </span><span class="kw">predict</span>(rf_brmodel, ds<span class="op">$</span>test) <span class="co">#prediction for BR +RF</span>
temp_rf &lt;-
<span class="kw">multilabel_evaluate</span>(ds<span class="op">$</span>test, rf_prediction, <span class="st">&quot;bipartition&quot;</span>) <span class="co">#get evaluation metric score for BR +RF</span>

<span class="co">#remove memory consuming variables</span>
<span class="kw">rm</span>(rf_brmodel)
<span class="kw">rm</span>(rf_prediction)

## XGB - eXtreme Gradient Boosting
xgb_brmodel &lt;-<span class="st"> </span><span class="kw">br</span>(ds<span class="op">$</span>train, <span class="st">&quot;XGB&quot;</span>) <span class="co">#create model - BR +XGB</span>
xgb_prediction &lt;-<span class="st"> </span><span class="kw">predict</span>(xgb_brmodel, ds<span class="op">$</span>test) <span class="co">#prediction for BR +XGB</span>
temp_xgb &lt;-
<span class="kw">multilabel_evaluate</span>(ds<span class="op">$</span>test, xgb_prediction, <span class="st">&quot;bipartition&quot;</span>) <span class="co">#get evaluation metric score  for BR +XGB</span>

<span class="co">#remove memory consuming variables</span>
<span class="kw">rm</span>(xgb_brmodel)
<span class="kw">rm</span>(xgb_prediction)

<span class="co">#create dataframe of evaluation metric scores</span>
<span class="cf">if</span> (index <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {
knn &lt;-<span class="st"> </span>temp_knn
rf &lt;-<span class="st"> </span>temp_rf
xgb &lt;-<span class="st"> </span>temp_xgb
} <span class="cf">else</span>{
knn &lt;-<span class="st"> </span><span class="kw">cbind</span>(knn, temp_knn)
rf &lt;-<span class="st"> </span><span class="kw">cbind</span>(rf, temp_rf)
xgb &lt;-<span class="st"> </span><span class="kw">cbind</span>(xgb, temp_xgb)
}</code></pre></div>
</div>
<div id="experimental-results" class="section level4">
<h4><span class="sub-header">Experimental Results</span></h4>
<p>We have applied the nine models over two languages (English and German) and with two types of features (TF-IDF and the terms incidence).<br />
For the purpose of the evaluation task, The <strong>mldr</strong> package equipped us with <em>multilabel_evaluate</em> method to inspect many evaluation metrics(accuracy, micro based metrics, macro based metrics, precision, recall, F1, subset-accuracy). We will present all of them for each experiment. Through the exploration process of the Eur-Lex dataset, we come to know the class labels are imbalanced (i.e. some labels are frequent and some are infrequent). In that case considering accuracy is a misleading measure of the performance, instead we consider <strong>F1</strong> as a comparision factor among the classifiers. Infact we consider <strong>macro-F1</strong> as the Macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), which is desirable in case of class imbalance.</p>
<div id="dataset-english" class="section level5">
<h5><span class="sub-header">Dataset: English</span></h5>
<p>This section answers three research questions for the English corpus:</p>
<ul>
<li><span class="emphasize">How well the classifiers perform over Eur-Lex dataset?</span><br />
</li>
<li><span class="emphasize">How the classifiers’ performance changes with different features- one with term frequency-inverse document frequency(tf-idf), another with term incidence?</span></li>
<li><span class="emphasize">Which flavour of multilabel transform algorithm perform best among all, the one which considers label correlation or the one which does not?</span></li>
</ul>
<p>For the English dataset, we observed higher macro F1 over all the nine trained classifiers when we used tf-idf as the features. Tf-idf is more powerful representative features than simply using the incidence of terms as features. The expreiments portrayed that LP combined with the Random Forest recorded the best result for the two type of features(tf-idf and term incidence), whereas the LP method combined with XGB performed the worst as shown in the following figures:</p>
<div class="figure">
<img src="Figs/English_7000.png" />

</div>
<div class="figure">
<img src="Figs/English_7000_table.png" />

</div>
<p>In the following sections we will display the results in detail for each dataset.</p>
</div>
<div id="dataset-english-feature-tf-idf" class="section level5 tabset tabset-fade">
<h5><span class="sub-sub-header">Dataset: English, Feature: Tf-idf</span></h5>
<div id="label-powerset" class="section level6">
<h6>Label Powerset</h6>
<p>According to the following figures, LP with KNN and Random Forest performed the best, and XGBoost performed the worst.</p>
<div class="figure">
<img src="Figs/lp_EN_tfidf1.png" />

</div>
<div class="figure">
<img src="Figs/lp_EN_tfidf2.png" />

</div>
</div>
<div id="binary-relevance" class="section level6">
<h6>Binary Relevance</h6>
<p>We compared the three models where the labels are assumed to be independent. The BR method produced the best results when combined with XGBoost.</p>
<div class="figure">
<img src="Figs/br_EN_tfidf1.png" />

</div>
<div class="figure">
<img src="Figs/br_EN_tfidf2.png" />

</div>
</div>
<div id="classifier-chain" class="section level6">
<h6>Classifier Chain</h6>
<p>Similar to the BR method, CC method performed the best with the XGBoost classifier.</p>
<div class="figure">
<img src="Figs/cc_EN_tfidf1.png" />

</div>
<div class="figure">
<img src="Figs/cc_EN_tfidf2.png" />

</div>
</div>
</div>
<div id="dataset-english-feature-term-incidence" class="section level5 tabset tabset-fade">
<h5><span class="sub-sub-header">Dataset: English, Feature: Term incidence</span></h5>
<p>We wanted to test the performance of the models in case of employing more naive features as the incidence of terms. The experiments showed similar pattern to the experiments with tf-idf dataset-LP with the Random Forest scored the highest macro F1 value, with the XGBoost performing the worst. Nevertheless, classifiers trained on the incidence of terms as features did not score better than the ones trained on the tf-idf features.</p>
<div id="label-powerset-1" class="section level6">
<h6>Label Powerset</h6>
<div class="figure">
<img src="Figs/lp_EN_inc1.png" />

</div>
<div class="figure">
<img src="Figs/lp_EN_inc2.png" />

</div>
</div>
<div id="binary-relevance-1" class="section level6">
<h6>Binary Relevance</h6>
<div class="figure">
<img src="Figs/br_EN_inc1.png" />

</div>
<div class="figure">
<img src="Figs/br_EN_inc2.png" />

</div>
</div>
<div id="classifier-chain-1" class="section level6">
<h6>Classifier Chain</h6>
<div class="figure">
<img src="Figs/cc_EN_inc1.png" />

</div>
<div class="figure">
<img src="Figs/cc_EN_inc2.png" />

</div>
</div>
</div>
<div id="dataset-german" class="section level5 tabset tabset-fade">
<h5><span class="sub-header">Dataset: German</span></h5>
<p>This section answers three research questions for the English corpus:</p>
<ul>
<li><span class="emphasize">How well the classifiers perform over Eur-Lex dataset?</span><br />
</li>
<li><span class="emphasize">How the classifiers’ performance changes with different features- one with term frequency-inverse document frequency(tf-idf), another with term incidence?</span></li>
<li><span class="emphasize">Which flavour of multilabel transform algorithm perform best among all, the one which considers label correlation or the one which does not?</span></li>
</ul>
<p>Similar to the experiments run over the English dataset, for the German dataset the experiments showed that LP combined with Random Forest or KNN recorded the best results for the two type of features(tf-idf and incidence of terms), compared to the low performance produced by the LP method combined with XGBoost. On the other hand, in the contrast to the results for the English dataset,for the German dataset, we observed higher macro F1 over all the nine trained classifiers when we used incidence of terms instead of the tf-idf as features.</p>
<div class="figure">
<img src="Figs/German_7000.png" />

</div>
<div class="figure">
<img src="Figs/German_7000_table.png" />

</div>
<p>In the following sections we will display the results of the experiments run on the German dataset in detail for each type of features.</p>
</div>
<div id="dataset-german-feature-tf-idf" class="section level5 tabset tabset-fade">
<h5><span class="sub-sub-header">Dataset: German, Feature: Tf-idf</span></h5>
<p>Unlike the experiments conducted on the English dataset, LP combined with KNN delivered slightly higher performance than the model combining the LP with Random Forest.</p>
<div id="label-powerset-2" class="section level6">
<h6>Label Powerset</h6>
<p><img src="Figs/lp_DE_tfidf1.png" /> <img src="Figs/lp_DE_tfidf2.png" /></p>
</div>
<div id="binary-relevance-2" class="section level6">
<h6>Binary Relevance</h6>
<p>The performance of BR was significantly poor over the three models. <img src="Figs/br_DE_tfidf1.png" /> <img src="Figs/br_DE_tfidf2.png" /></p>
</div>
</div>
<div id="dataset-german-dataset-feature-incidence-of-terms" class="section level5 tabset tabset-fade">
<h5><span class="sub-sub-header">Dataset: German dataset, Feature: Incidence of terms</span></h5>
<p>The classifiers performed better over term incidence features, which portrays term incidence are good discriminatory features compared tf-idf, which demands more computational effort compared to term incidence.</p>
<div id="label-powerset-3" class="section level6">
<h6>Label Powerset</h6>
<div class="figure">
<img src="Figs/lp_DE_inc1.png" />

</div>
<div class="figure">
<img src="Figs/lp_DE_inc2.png" />

</div>
</div>
<div id="binary-relevance-3" class="section level6">
<h6>Binary Relevance</h6>
<div class="figure">
<img src="Figs/br_DE_inc1.png" />

</div>
<div class="figure">
<img src="Figs/br_DE_inc2.png" />

</div>
</div>
</div>
<div id="dataset-with-balanced-labelsets" class="section level5">
<h5><span class="sub-header">Dataset with balanced labelsets</h5>
<p>Training our classification models on a dataset with such a large number of labels(7000 labels) was a challenging task. we surmised that reducing the number of labels would improve the predictive capacity of the classifiers. To reduce the large number of labels, we take advantage of the “scumble” attribute provided by the <strong>mldr</strong> package. Scumble measure indicates the concurrence level among frequent and infrequent labels in the same labelsets. we simply removed imbalanced labelsets(i.e imbalanced labelsets are labelsets with frequent and infrequent labels) and kept only balanced labelsets by filtering only labelsets with lower level of scumble values than the mean scumble value of the dataset with the following command line:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">datasetWithBalancedLabelsets &lt;-<span class="st"> </span>dataset[.SCUMBLE <span class="op">&lt;=</span><span class="st"> </span>dataset<span class="op">$</span>measures<span class="op">$</span>scumble]</code></pre></div>
</div>
<div id="dataset-english-balanced-labelsets" class="section level5">
<h5><span class="sub-sub-header">Dataset: English, Balanced labelsets</span></h5>
<p>This section the answers research question for the English corpus:<br />
<span class="emphasize">How the classifiers’ performance changes when the number of labels is reduced, and which features delivered the best performance.</span></p>
<p>Some classifiers scored higher than the highest macro F1 value scored in the case of keeping imbalanced labelsets. Employing the tf-idf features yielded higher macro F1 for classifiers trained on the complete set of labels. In the case of balanced labelsets, classifiers trained on the incidence of terms showed slightly better performance as shown in the following graphs:</p>
<div class="figure">
<img src="Figs/English_reduced.png" />

</div>
<div class="figure">
<img src="Figs/English_reduced_table.png" />

</div>
</div>
<div id="dataset-english-feature-tf-idf-balanced-labelsets" class="section level5 tabset tabset-fade">
<h5><span class="sub-sub-header"> Dataset: English, Feature: Tf-idf, Balanced labelsets</span></h5>
<p>For the tf-idf features, most of the classifiers trained on the balanced labelsets maintained almost similar levels of performance compared to the performace with the complete set of labels.</p>
<div id="label-powerset-4" class="section level6">
<h6>Label Powerset</h6>
<div class="figure">
<img src="Figs/lp_EN_tfidf1_red.png" />

</div>
<div class="figure">
<img src="Figs/lp_EN_tfidf2_red.png" />

</div>
</div>
<div id="binary-relevance-4" class="section level6">
<h6>Binary Relevance</h6>
<div class="figure">
<img src="Figs/br_EN_tfidf1_red.png" />

</div>
<div class="figure">
<img src="Figs/br_EN_tfidf2_red.png" />

</div>
</div>
</div>
<div id="dataset-english-feature-term-incidence-balanced-labelsets" class="section level5 tabset tabset-fade">
<h5><span class="sub-sub-header">Dataset: English, Feature: Term incidence, balanced labelsets</span></h5>
<p>For balanced labelsets, we observed better performance for the incidence features for all classifiers compared to the performance with the complete labelsets. LP combined with the Random Forest classifier outperformed the performance of the same classification model trained on the complete set with the tf-idf features. We consider incidence of terms as sufficient features even for the English dataset, after pruning the imbalanced labelsets.</p>
<div id="label-powerset-5" class="section level6">
<h6>Label Powerset</h6>
<div class="figure">
<img src="Figs/lp_EN_inc1_red.png" />

</div>
<div class="figure">
<img src="Figs/lp_EN_inc2_red.png" />

</div>
</div>
<div id="binary-relevance-5" class="section level6">
<h6>Binary Relevance</h6>
<div class="figure">
<img src="Figs/br_EN_inc1_red.png" />

</div>
<div class="figure">
<img src="Figs/br_EN_inc2_red.png" />

</div>
</div>
</div>
<div id="dataset-german-balanced-labelsets" class="section level5">
<h5><span class="sub-sub-header">Dataset: German, Balanced labelsets</span></h5>
<p>This section answers the research question for the German corpus:<br />
<span class="emphasize">How the classifiers’ performance changes when the number of labels is reduced, and which features delivered the best performance.</span></p>
<div class="figure">
<img src="Figs/German_reduced2.png" />

</div>
</div>
<div id="dataset-german-feature-tf-idf-balanced-labelsets" class="section level5">
<h5><span class="sub-sub-header">Dataset: German, Feature: Tf-idf, Balanced labelsets</span></h5>
<p>After removing the imbalanced labelsets, the macro F1 for the combination of LP with Random Forest increased by around 15%.</p>
<div class="figure">
<img src="Figs/lp_DE_tfidf1_red.png" />

</div>
<div class="figure">
<img src="Figs/lp_DE_tfidf2_red.png" />

</div>
</div>
<div id="dataset-german-feature-term-incidence-balanced-labelsets" class="section level5">
<h5><span class="sub-sub-header">Dataset: German, Feature: Term incidence, balanced labelsets</span></h5>
<div class="figure">
<img src="Figs/lp_DE_rf_inc2_red.png" />

</div>
</div>
</div>
<div id="final-analysis" class="section level4">
<h4><span class="sub-header">Final Analysis</span></h4>
<blockquote>
<blockquote>
<p>We can conclude for both English and German dataset,the best performance was delivered by the LP transform combined with the Random Forest classifier after removing imbalanced labelsets through exploiting the scumble measure and trained on the terms incidence in order to gain the highest macro F1 values.</p>
</blockquote>
</blockquote>
<ul>
<li><p>Based on the results of various experiments that we conducted on the different classification models, the best performance was observed for the combination <span class="emphasize">LP transform with Random Forest</span> and <span class="emphasize">LP transform with K-nearest neighbour</span>, compared to BR transform along with the same classifiers .</p></li>
<li><p>As mentioned earlier, LP transform takes into consideration the correlation among the labels. In contrast, the BR transform assumes the labels to be independent and ignores any dependency among labels. We infer that the assumption of the independency of labels does not hold in the case of the EUR-Lex dataset.</p></li>
<li><p>Contrary to the LP method,each model in the BR transform considers the labels independently- it reduces the number of labels when applying the BR to build an independent classifier for each label, which improves the perfomance of the XGBoost classifier by the model optimization technique the XGB follows.</p></li>
<li><p>In experiments over the BR transformation method, XGBoost performed better than the Random Forest and the k-nearest neighbour classifiers. We surmise, since XGBoost classifier is an ensemble model uses “Boosting” as a deliberate ensemble technique, whereas Random Forest employs “Bagging” as an ensemble technique and KNN encounters the curse of dimensionality.</p></li>
<li><p>CC transform method delivered poor performance across all classication models(k-nearest neighbour, Random Forest, XGBoost). XGBoost performed the best with CC, due to the boosting optimization technique the XGBoost based on.</p></li>
<li><p>Suprisingly, we found the classifiers trained on the <em>terms incidence</em> features performed better than when trained on the <em>tf-idf</em> features. we think that the occurance of the term in a document was a sufficient distinctive feature of the documents of the legal EUR-Lex dataset.</p></li>
<li><p>With balanced labelsets we found that the classifiers overall performed better, though not much, for both English and German dataset.</p></li>
</ul>
</div>
<div id="conclusion" class="section level4">
<h4><span class="sub-header">Conclusion</span></h4>
<p>The best score for macro-F1/F1 we could get for the English dataset is 0.265/0.365 and for German dataset is 0.269/0.458, and the publications using the earlier version of the dataset having 4000 labels also reported a poor F1 score less than 0.5. It implies that the features which we have used might not have behaved as discriminatory as expected, but we cannot rule out the bias of the annotators, which might have prevented us to get good results. The annotators categorizing the document might have been influenced by various aspects as mentioned in literature <span class="citation">[<a href="#ref-rabiger2018annotators">1</a>]</span> while assigning them to the categories. Therefore it seems challenging for the machine learning algorithms to match with the human annotated categories.</p>
<div id="references" class="section level5 unnumbered">
<h5><span class="sub-header">References</span></h5>
<div id="refs" class="references">
<div id="ref-rabiger2018annotators">
<p>1. Räbiger S, Spiliopoulou M, Saygına Y. How do annotators label short texts? Toward understanding the temporal dynamics of tweet labeling. 2018.</p>
</div>
</div>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
